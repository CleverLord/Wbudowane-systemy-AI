{"cells":[{"cell_type":"markdown","metadata":{"id":"wHGC0EFt3aZF"},"source":["# Binary Neural Networks\n","\n","Existing deep neural networks use 32 bits, 16 bits or 8 bits to encode each weight and activation, making them large, slow and power-hungry. This prohibits many applications in resource-constrained environments.\n","\n","Binary Neural Network is a type of neural network that activations (or called features) and weights are 1-bit values in all the hidden layers (except the input and output layers). In a few words, BNN is an extremely compacted case of CNN - they have the same structures except for the different precision activations and weights. The use of BNNs enables not only significant reduction in memory usage, but also huge computational complexity improvements - the replacement of multiply-accumulation operations by `XNOR` and `bitcount` operations.\n","\n","Today, we'll use `Larq` - an open-source Python library for training neural networks with extremely low-precision weights and activations.\n","\n","You can find out more here: https://docs.larq.dev/larq/\n","\n","Larq is build on top of `TensorFlow` which is Python library for training neural networks, just like `PyTorch`. Because of that, we need to get familiar with this framework.\n","\n","First, install and import nessessery libraries."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"3fkUb7mzgmGD"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: larq in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (0.13.1)\n","Requirement already satisfied: larq-zoo in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (2.3.2)\n","Requirement already satisfied: larq-compute-engine in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (0.13.0)\n","Requirement already satisfied: numpy<2.0,>=1.15.4 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from larq) (1.24.4)\n","Requirement already satisfied: terminaltables>=3.1.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from larq) (3.1.10)\n","Requirement already satisfied: packaging>=19.2 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from larq) (23.2)\n","Requirement already satisfied: zookeeper>=1.0.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from larq-zoo) (1.3.4)\n","Requirement already satisfied: typeguard<3.0.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from larq-zoo) (2.13.3)\n","Requirement already satisfied: protobuf<3.20 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from larq-zoo) (3.19.6)\n","Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from larq-compute-engine) (23.5.26)\n","Requirement already satisfied: tqdm>=4 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from larq-compute-engine) (4.66.1)\n","Requirement already satisfied: colorama in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tqdm>=4->larq-compute-engine) (0.4.6)\n","Requirement already satisfied: click>=7.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from zookeeper>=1.0.0->larq-zoo) (8.1.7)\n","Requirement already satisfied: tensorflow-datasets<v4.9.0,>=1.3.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from zookeeper>=1.0.0->larq-zoo) (4.8.3)\n","Requirement already satisfied: absl-py in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-datasets<v4.9.0,>=1.3.0->zookeeper>=1.0.0->larq-zoo) (1.4.0)\n","Requirement already satisfied: dm-tree in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-datasets<v4.9.0,>=1.3.0->zookeeper>=1.0.0->larq-zoo) (0.1.8)\n","Requirement already satisfied: etils>=0.9.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets<v4.9.0,>=1.3.0->zookeeper>=1.0.0->larq-zoo) (1.5.2)\n","Requirement already satisfied: promise in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-datasets<v4.9.0,>=1.3.0->zookeeper>=1.0.0->larq-zoo) (2.3)\n","Requirement already satisfied: psutil in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-datasets<v4.9.0,>=1.3.0->zookeeper>=1.0.0->larq-zoo) (5.9.6)\n","Requirement already satisfied: requests>=2.19.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-datasets<v4.9.0,>=1.3.0->zookeeper>=1.0.0->larq-zoo) (2.31.0)\n","Requirement already satisfied: tensorflow-metadata in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-datasets<v4.9.0,>=1.3.0->zookeeper>=1.0.0->larq-zoo) (1.13.0)\n","Requirement already satisfied: termcolor in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-datasets<v4.9.0,>=1.3.0->zookeeper>=1.0.0->larq-zoo) (2.3.0)\n","Requirement already satisfied: toml in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-datasets<v4.9.0,>=1.3.0->zookeeper>=1.0.0->larq-zoo) (0.10.2)\n","Requirement already satisfied: wrapt in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-datasets<v4.9.0,>=1.3.0->zookeeper>=1.0.0->larq-zoo) (1.15.0)\n","Requirement already satisfied: fsspec in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets<v4.9.0,>=1.3.0->zookeeper>=1.0.0->larq-zoo) (2023.10.0)\n","Requirement already satisfied: importlib_resources in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets<v4.9.0,>=1.3.0->zookeeper>=1.0.0->larq-zoo) (6.1.0)\n","Requirement already satisfied: typing_extensions in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets<v4.9.0,>=1.3.0->zookeeper>=1.0.0->larq-zoo) (4.8.0)\n","Requirement already satisfied: zipp in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets<v4.9.0,>=1.3.0->zookeeper>=1.0.0->larq-zoo) (3.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets<v4.9.0,>=1.3.0->zookeeper>=1.0.0->larq-zoo) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets<v4.9.0,>=1.3.0->zookeeper>=1.0.0->larq-zoo) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets<v4.9.0,>=1.3.0->zookeeper>=1.0.0->larq-zoo) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets<v4.9.0,>=1.3.0->zookeeper>=1.0.0->larq-zoo) (2023.7.22)\n","Requirement already satisfied: six in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from promise->tensorflow-datasets<v4.9.0,>=1.3.0->zookeeper>=1.0.0->larq-zoo) (1.16.0)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-metadata->tensorflow-datasets<v4.9.0,>=1.3.0->zookeeper>=1.0.0->larq-zoo) (1.61.0)\n","Collecting tensorflow==2.10.0\n","  Downloading tensorflow-2.10.0-cp39-cp39-win_amd64.whl (455.9 MB)\n","     ---------------------------------------- 0.0/455.9 MB ? eta -:--:--\n","     ---------------------------------------- 0.1/455.9 MB 3.3 MB/s eta 0:02:18\n","     ---------------------------------------- 0.5/455.9 MB 6.2 MB/s eta 0:01:14\n","     --------------------------------------- 1.6/455.9 MB 12.7 MB/s eta 0:00:36\n","     --------------------------------------- 4.6/455.9 MB 26.7 MB/s eta 0:00:17\n","      -------------------------------------- 6.8/455.9 MB 30.8 MB/s eta 0:00:15\n","      -------------------------------------- 7.5/455.9 MB 28.2 MB/s eta 0:00:16\n","      -------------------------------------- 8.8/455.9 MB 29.4 MB/s eta 0:00:16\n","      ------------------------------------- 10.2/455.9 MB 29.5 MB/s eta 0:00:16\n","      ------------------------------------- 11.7/455.9 MB 38.6 MB/s eta 0:00:12\n","     - ------------------------------------ 13.6/455.9 MB 34.4 MB/s eta 0:00:13\n","     - ------------------------------------ 14.6/455.9 MB 34.4 MB/s eta 0:00:13\n","     - ------------------------------------ 16.1/455.9 MB 28.4 MB/s eta 0:00:16\n","     - ------------------------------------ 18.3/455.9 MB 36.4 MB/s eta 0:00:13\n","     - ------------------------------------ 22.7/455.9 MB 46.9 MB/s eta 0:00:10\n","     -- ----------------------------------- 26.4/455.9 MB 81.8 MB/s eta 0:00:06\n","     -- ----------------------------------- 29.9/455.9 MB 93.9 MB/s eta 0:00:05\n","     -- ----------------------------------- 33.5/455.9 MB 72.6 MB/s eta 0:00:06\n","     --- ---------------------------------- 37.5/455.9 MB 81.8 MB/s eta 0:00:06\n","     --- ---------------------------------- 41.0/455.9 MB 72.6 MB/s eta 0:00:06\n","     --- ---------------------------------- 44.9/455.9 MB 81.8 MB/s eta 0:00:06\n","     ---- --------------------------------- 49.0/455.9 MB 81.8 MB/s eta 0:00:05\n","     ---- --------------------------------- 52.5/455.9 MB 81.8 MB/s eta 0:00:05\n","     ---- --------------------------------- 56.4/455.9 MB 81.8 MB/s eta 0:00:05\n","     ----- -------------------------------- 60.3/455.9 MB 81.8 MB/s eta 0:00:05\n","     ----- -------------------------------- 64.1/455.9 MB 93.9 MB/s eta 0:00:05\n","     ----- -------------------------------- 68.2/455.9 MB 93.9 MB/s eta 0:00:05\n","     ----- -------------------------------- 70.9/455.9 MB 81.8 MB/s eta 0:00:05\n","     ------ ------------------------------- 74.6/455.9 MB 72.6 MB/s eta 0:00:06\n","     ------ ------------------------------- 78.5/455.9 MB 72.6 MB/s eta 0:00:06\n","     ------ ------------------------------- 82.5/455.9 MB 81.8 MB/s eta 0:00:05\n","     ------- ------------------------------ 86.4/455.9 MB 81.8 MB/s eta 0:00:05\n","     ------- ------------------------------ 90.3/455.9 MB 81.8 MB/s eta 0:00:05\n","     ------- ------------------------------ 94.3/455.9 MB 81.8 MB/s eta 0:00:05\n","     -------- ----------------------------- 98.5/455.9 MB 81.8 MB/s eta 0:00:05\n","     -------- ---------------------------- 102.2/455.9 MB 81.8 MB/s eta 0:00:05\n","     -------- ---------------------------- 105.9/455.9 MB 93.9 MB/s eta 0:00:04\n","     -------- ---------------------------- 108.4/455.9 MB 72.6 MB/s eta 0:00:05\n","     --------- --------------------------- 112.6/455.9 MB 72.6 MB/s eta 0:00:05\n","     --------- --------------------------- 116.1/455.9 MB 65.2 MB/s eta 0:00:06\n","     --------- --------------------------- 120.3/455.9 MB 81.8 MB/s eta 0:00:05\n","     ---------- -------------------------- 124.3/455.9 MB 93.0 MB/s eta 0:00:04\n","     ---------- -------------------------- 128.5/455.9 MB 93.0 MB/s eta 0:00:04\n","     ---------- -------------------------- 132.5/455.9 MB 93.0 MB/s eta 0:00:04\n","     ---------- -------------------------- 133.2/455.9 MB 93.9 MB/s eta 0:00:04\n","     ---------- -------------------------- 133.2/455.9 MB 93.9 MB/s eta 0:00:04\n","     ---------- -------------------------- 133.2/455.9 MB 93.9 MB/s eta 0:00:04\n","     ---------- -------------------------- 133.2/455.9 MB 93.9 MB/s eta 0:00:04\n","     ---------- -------------------------- 133.2/455.9 MB 93.9 MB/s eta 0:00:04\n","     ---------- -------------------------- 134.7/455.9 MB 28.4 MB/s eta 0:00:12\n","     ----------- ------------------------- 138.7/455.9 MB 27.3 MB/s eta 0:00:12\n","     ----------- ------------------------- 142.5/455.9 MB 27.3 MB/s eta 0:00:12\n","     ----------- ------------------------- 146.3/455.9 MB 81.8 MB/s eta 0:00:04\n","     ------------ ------------------------ 149.1/455.9 MB 73.1 MB/s eta 0:00:05\n","     ------------ ------------------------ 149.9/455.9 MB 72.6 MB/s eta 0:00:05\n","     ------------ ------------------------ 150.7/455.9 MB 46.7 MB/s eta 0:00:07\n","     ------------ ------------------------ 152.4/455.9 MB 43.7 MB/s eta 0:00:07\n","     ------------ ------------------------ 154.2/455.9 MB 38.5 MB/s eta 0:00:08\n","     ------------ ------------------------ 155.0/455.9 MB 34.4 MB/s eta 0:00:09\n","     ------------ ------------------------ 156.4/455.9 MB 31.1 MB/s eta 0:00:10\n","     ------------ ------------------------ 157.0/455.9 MB 28.5 MB/s eta 0:00:11\n","     ------------ ------------------------ 158.0/455.9 MB 26.2 MB/s eta 0:00:12\n","     ------------ ------------------------ 159.2/455.9 MB 24.2 MB/s eta 0:00:13\n","     ------------- ----------------------- 160.5/455.9 MB 27.3 MB/s eta 0:00:11\n","     ------------- ----------------------- 162.4/455.9 MB 26.2 MB/s eta 0:00:12\n","     ------------- ----------------------- 165.2/455.9 MB 32.8 MB/s eta 0:00:09\n","     ------------- ----------------------- 169.2/455.9 MB 54.7 MB/s eta 0:00:06\n","     -------------- ---------------------- 173.3/455.9 MB 81.8 MB/s eta 0:00:04\n","     -------------- ---------------------- 177.5/455.9 MB 93.0 MB/s eta 0:00:03\n","     -------------- ---------------------- 181.6/455.9 MB 93.0 MB/s eta 0:00:03\n","     -------------- ---------------------- 183.5/455.9 MB 81.8 MB/s eta 0:00:04\n","     -------------- ---------------------- 183.5/455.9 MB 81.8 MB/s eta 0:00:04\n","     -------------- ---------------------- 183.5/455.9 MB 81.8 MB/s eta 0:00:04\n","     -------------- ---------------------- 183.5/455.9 MB 81.8 MB/s eta 0:00:04\n","     -------------- ---------------------- 184.7/455.9 MB 32.8 MB/s eta 0:00:09\n","     --------------- --------------------- 188.8/455.9 MB 32.7 MB/s eta 0:00:09\n","     --------------- --------------------- 193.1/455.9 MB 32.7 MB/s eta 0:00:09\n","     ---------------- -------------------- 197.2/455.9 MB 93.0 MB/s eta 0:00:03\n","     ---------------- -------------------- 200.6/455.9 MB 81.8 MB/s eta 0:00:04\n","     ---------------- -------------------- 203.4/455.9 MB 81.8 MB/s eta 0:00:04\n","     ---------------- -------------------- 203.4/455.9 MB 81.8 MB/s eta 0:00:04\n","     ---------------- -------------------- 203.4/455.9 MB 81.8 MB/s eta 0:00:04\n","     ---------------- -------------------- 203.4/455.9 MB 81.8 MB/s eta 0:00:04\n","     ---------------- -------------------- 205.6/455.9 MB 34.4 MB/s eta 0:00:08\n","     ----------------- ------------------- 209.9/455.9 MB 34.4 MB/s eta 0:00:08\n","     ----------------- ------------------- 214.1/455.9 MB 72.6 MB/s eta 0:00:04\n","     ----------------- ------------------- 218.1/455.9 MB 93.0 MB/s eta 0:00:03\n","     ----------------- ------------------- 221.7/455.9 MB 93.0 MB/s eta 0:00:03\n","     ------------------ ------------------ 226.5/455.9 MB 93.9 MB/s eta 0:00:03\n","     ------------------ ------------------ 230.4/455.9 MB 81.8 MB/s eta 0:00:03\n","     ------------------- ----------------- 234.7/455.9 MB 93.9 MB/s eta 0:00:03\n","     ------------------- ----------------- 239.1/455.9 MB 81.8 MB/s eta 0:00:03\n","     ------------------- ----------------- 242.2/455.9 MB 93.9 MB/s eta 0:00:03\n","     ------------------- ----------------- 246.3/455.9 MB 81.8 MB/s eta 0:00:03\n","     -------------------- ---------------- 250.5/455.9 MB 81.8 MB/s eta 0:00:03\n","     -------------------- --------------- 254.9/455.9 MB 108.8 MB/s eta 0:00:02\n","     -------------------- --------------- 259.3/455.9 MB 108.8 MB/s eta 0:00:02\n","     -------------------- --------------- 263.7/455.9 MB 108.8 MB/s eta 0:00:02\n","     --------------------- --------------- 268.0/455.9 MB 93.9 MB/s eta 0:00:03\n","     --------------------- -------------- 272.4/455.9 MB 110.0 MB/s eta 0:00:02\n","     ---------------------- -------------- 276.8/455.9 MB 93.9 MB/s eta 0:00:02\n","     ---------------------- -------------- 280.8/455.9 MB 93.9 MB/s eta 0:00:02\n","     ----------------------- ------------- 284.5/455.9 MB 93.9 MB/s eta 0:00:02\n","     ----------------------- ------------- 289.3/455.9 MB 93.0 MB/s eta 0:00:02\n","     ----------------------- ------------- 293.4/455.9 MB 93.0 MB/s eta 0:00:02\n","     ------------------------ ------------ 297.9/455.9 MB 93.0 MB/s eta 0:00:02\n","     ------------------------ ------------ 302.0/455.9 MB 93.9 MB/s eta 0:00:02\n","     ------------------------ ------------ 306.4/455.9 MB 93.9 MB/s eta 0:00:02\n","     ------------------------- ----------- 310.6/455.9 MB 93.9 MB/s eta 0:00:02\n","     ------------------------- ----------- 315.0/455.9 MB 93.9 MB/s eta 0:00:02\n","     ------------------------- ----------- 319.5/455.9 MB 93.9 MB/s eta 0:00:02\n","     ------------------------- ---------- 323.3/455.9 MB 108.8 MB/s eta 0:00:02\n","     -------------------------- ---------- 327.8/455.9 MB 93.0 MB/s eta 0:00:02\n","     -------------------------- ---------- 331.9/455.9 MB 93.9 MB/s eta 0:00:02\n","     -------------------------- --------- 336.1/455.9 MB 108.8 MB/s eta 0:00:02\n","     -------------------------- --------- 340.8/455.9 MB 108.8 MB/s eta 0:00:02\n","     ---------------------------- -------- 345.0/455.9 MB 93.0 MB/s eta 0:00:02\n","     --------------------------- -------- 349.5/455.9 MB 110.0 MB/s eta 0:00:01\n","     ---------------------------- -------- 353.7/455.9 MB 93.9 MB/s eta 0:00:02\n","     ----------------------------- ------- 357.6/455.9 MB 93.9 MB/s eta 0:00:02\n","     ----------------------------- ------- 359.5/455.9 MB 72.6 MB/s eta 0:00:02\n","     ----------------------------- ------- 363.8/455.9 MB 72.6 MB/s eta 0:00:02\n","     ----------------------------- ------- 368.3/455.9 MB 93.0 MB/s eta 0:00:01\n","     ------------------------------ ------ 372.5/455.9 MB 93.0 MB/s eta 0:00:01\n","     ------------------------------ ------ 376.9/455.9 MB 93.0 MB/s eta 0:00:01\n","     ------------------------------ ------ 380.7/455.9 MB 93.9 MB/s eta 0:00:01\n","     ------------------------------- ----- 384.8/455.9 MB 81.8 MB/s eta 0:00:01\n","     ------------------------------- ----- 389.5/455.9 MB 81.8 MB/s eta 0:00:01\n","     ------------------------------- ----- 393.9/455.9 MB 93.9 MB/s eta 0:00:01\n","     -------------------------------- ---- 398.4/455.9 MB 93.9 MB/s eta 0:00:01\n","     -------------------------------- ---- 402.8/455.9 MB 93.0 MB/s eta 0:00:01\n","     -------------------------------- --- 407.1/455.9 MB 108.8 MB/s eta 0:00:01\n","     -------------------------------- --- 411.6/455.9 MB 108.8 MB/s eta 0:00:01\n","     -------------------------------- --- 416.2/455.9 MB 108.8 MB/s eta 0:00:01\n","     --------------------------------- -- 419.4/455.9 MB 108.8 MB/s eta 0:00:01\n","     --------------------------------- -- 419.4/455.9 MB 108.8 MB/s eta 0:00:01\n","     --------------------------------- -- 419.4/455.9 MB 108.8 MB/s eta 0:00:01\n","     --------------------------------- -- 419.4/455.9 MB 108.8 MB/s eta 0:00:01\n","     --------------------------------- -- 419.4/455.9 MB 108.8 MB/s eta 0:00:01\n","     ---------------------------------- -- 420.5/455.9 MB 34.4 MB/s eta 0:00:02\n","     ---------------------------------- -- 420.5/455.9 MB 34.4 MB/s eta 0:00:02\n","     ---------------------------------- -- 423.6/455.9 MB 26.2 MB/s eta 0:00:02\n","     ---------------------------------- -- 423.6/455.9 MB 26.2 MB/s eta 0:00:02\n","     ---------------------------------- -- 424.1/455.9 MB 21.1 MB/s eta 0:00:02\n","     ---------------------------------- -- 428.6/455.9 MB 21.1 MB/s eta 0:00:02\n","     ----------------------------------- - 432.6/455.9 MB 50.4 MB/s eta 0:00:01\n","     ----------------------------------- - 436.5/455.9 MB 93.9 MB/s eta 0:00:01\n","     ----------------------------------- - 440.8/455.9 MB 93.9 MB/s eta 0:00:01\n","     ------------------------------------  445.2/455.9 MB 93.9 MB/s eta 0:00:01\n","     ------------------------------------  445.6/455.9 MB 93.9 MB/s eta 0:00:01\n","     ------------------------------------  450.1/455.9 MB 65.2 MB/s eta 0:00:01\n","     ------------------------------------  454.3/455.9 MB 65.2 MB/s eta 0:00:01\n","     ------------------------------------  455.9/455.9 MB 93.9 MB/s eta 0:00:01\n","     ------------------------------------  455.9/455.9 MB 93.9 MB/s eta 0:00:01\n","     ------------------------------------  455.9/455.9 MB 93.9 MB/s eta 0:00:01\n","     ------------------------------------  455.9/455.9 MB 93.9 MB/s eta 0:00:01\n","     ------------------------------------  455.9/455.9 MB 93.9 MB/s eta 0:00:01\n","     ------------------------------------  455.9/455.9 MB 93.9 MB/s eta 0:00:01\n","     ------------------------------------  455.9/455.9 MB 93.9 MB/s eta 0:00:01\n","     ------------------------------------  455.9/455.9 MB 93.9 MB/s eta 0:00:01\n","     ------------------------------------  455.9/455.9 MB 93.9 MB/s eta 0:00:01\n","     ------------------------------------  455.9/455.9 MB 93.9 MB/s eta 0:00:01\n","     ------------------------------------  455.9/455.9 MB 93.9 MB/s eta 0:00:01\n","     ------------------------------------  455.9/455.9 MB 93.9 MB/s eta 0:00:01\n","     ------------------------------------- 455.9/455.9 MB 15.6 MB/s eta 0:00:00\n","Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow==2.10.0) (1.4.0)\n","Collecting astunparse>=1.6.0 (from tensorflow==2.10.0)\n","  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n","Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow==2.10.0) (23.5.26)\n","Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.10.0)\n","  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Collecting google-pasta>=0.1.1 (from tensorflow==2.10.0)\n","  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n","     ---------------------------------------- 0.0/57.5 kB ? eta -:--:--\n","     ---------------------------------------- 57.5/57.5 kB ? eta 0:00:00\n","Collecting h5py>=2.9.0 (from tensorflow==2.10.0)\n","  Downloading h5py-3.10.0-cp39-cp39-win_amd64.whl.metadata (2.5 kB)\n","Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.10.0)\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","     ---------------------------------------- 0.0/42.6 kB ? eta -:--:--\n","     ---------------------------------------- 42.6/42.6 kB ? eta 0:00:00\n","Collecting libclang>=13.0.0 (from tensorflow==2.10.0)\n","  Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n","Requirement already satisfied: numpy>=1.20 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow==2.10.0) (1.24.4)\n","Collecting opt-einsum>=2.3.2 (from tensorflow==2.10.0)\n","  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n","     ---------------------------------------- 0.0/65.5 kB ? eta -:--:--\n","     ---------------------------------------- 65.5/65.5 kB ? eta 0:00:00\n","Requirement already satisfied: packaging in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow==2.10.0) (23.2)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow==2.10.0) (3.19.6)\n","Requirement already satisfied: setuptools in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow==2.10.0) (58.1.0)\n","Requirement already satisfied: six>=1.12.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow==2.10.0) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow==2.10.0) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow==2.10.0) (4.8.0)\n","Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow==2.10.0) (1.15.0)\n","Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow==2.10.0)\n","  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n","     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n","     ---------------------------------------- 1.5/1.5 MB 98.4 MB/s eta 0:00:00\n","Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.10.0)\n","  Downloading grpcio-1.59.2-cp39-cp39-win_amd64.whl.metadata (4.2 kB)\n","Collecting tensorboard<2.11,>=2.10 (from tensorflow==2.10.0)\n","  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n","     ---------------------------------------- 0.0/5.9 MB ? eta -:--:--\n","     ----------------------------- ---------- 4.4/5.9 MB 140.1 MB/s eta 0:00:01\n","     ---------------------------------------- 5.9/5.9 MB 75.2 MB/s eta 0:00:00\n","Collecting tensorflow-estimator<2.11,>=2.10.0 (from tensorflow==2.10.0)\n","  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n","     ---------------------------------------- 0.0/438.7 kB ? eta -:--:--\n","     ------------------------------------- 438.7/438.7 kB 26.8 MB/s eta 0:00:00\n","Collecting keras<2.11,>=2.10.0 (from tensorflow==2.10.0)\n","  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n","     ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n","     ---------------------------------------- 1.7/1.7 MB 54.0 MB/s eta 0:00:00\n","Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow==2.10.0)\n","  Using cached wheel-0.41.3-py3-none-any.whl.metadata (2.2 kB)\n","Collecting google-auth<3,>=1.6.3 (from tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n","  Downloading google_auth-2.23.4-py2.py3-none-any.whl.metadata (4.7 kB)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Collecting markdown>=2.6.8 (from tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n","  Downloading Markdown-3.5.1-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.31.0)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n","  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n","Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","     ---------------------------------------- 0.0/781.3 kB ? eta -:--:--\n","     ------------------------------------- 781.3/781.3 kB 48.2 MB/s eta 0:00:00\n","Collecting werkzeug>=1.0.1 (from tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n","  Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n","Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n","  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n","Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n","  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n","     ---------------------------------------- 0.0/181.3 kB ? eta -:--:--\n","     ---------------------------------------- 181.3/181.3 kB ? eta 0:00:00\n","Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n","  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n","Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n","  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n","Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (6.8.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.1.3)\n","Requirement already satisfied: zipp>=0.5 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.17.0)\n","Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n","  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n","     ---------------------------------------- 0.0/83.9 kB ? eta -:--:--\n","     ---------------------------------------- 83.9/83.9 kB ? eta 0:00:00\n","Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n","  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n","     ---------------------------------------- 0.0/151.7 kB ? eta -:--:--\n","     ---------------------------------------- 151.7/151.7 kB ? eta 0:00:00\n","Downloading grpcio-1.59.2-cp39-cp39-win_amd64.whl (3.7 MB)\n","   ---------------------------------------- 0.0/3.7 MB ? eta -:--:--\n","   ------------------------------- -------- 2.9/3.7 MB 61.0 MB/s eta 0:00:01\n","   ---------------------------------------- 3.7/3.7 MB 58.4 MB/s eta 0:00:00\n","Downloading h5py-3.10.0-cp39-cp39-win_amd64.whl (2.7 MB)\n","   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n","   ---------------------------------------  2.7/2.7 MB 86.9 MB/s eta 0:00:01\n","   ---------------------------------------- 2.7/2.7 MB 57.5 MB/s eta 0:00:00\n","Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n","   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n","   ---- ----------------------------------- 3.0/24.4 MB 64.1 MB/s eta 0:00:01\n","   ---------- ----------------------------- 6.3/24.4 MB 66.8 MB/s eta 0:00:01\n","   --------------- ------------------------ 9.5/24.4 MB 76.3 MB/s eta 0:00:01\n","   --------------------- ------------------ 12.9/24.4 MB 72.6 MB/s eta 0:00:01\n","   -------------------------- ------------- 16.0/24.4 MB 72.6 MB/s eta 0:00:01\n","   ------------------------------- -------- 19.4/24.4 MB 72.6 MB/s eta 0:00:01\n","   ------------------------------------- -- 22.8/24.4 MB 73.1 MB/s eta 0:00:01\n","   ---------------------------------------  24.4/24.4 MB 72.6 MB/s eta 0:00:01\n","   ---------------------------------------- 24.4/24.4 MB 59.4 MB/s eta 0:00:00\n","Downloading google_auth-2.23.4-py2.py3-none-any.whl (183 kB)\n","   ---------------------------------------- 0.0/183.3 kB ? eta -:--:--\n","   ---------------------------------------- 183.3/183.3 kB ? eta 0:00:00\n","Downloading Markdown-3.5.1-py3-none-any.whl (102 kB)\n","   ---------------------------------------- 0.0/102.2 kB ? eta -:--:--\n","   ---------------------------------------- 102.2/102.2 kB ? eta 0:00:00\n","Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n","   ---------------------------------------- 0.0/226.7 kB ? eta -:--:--\n","   --------------------------------------- 226.7/226.7 kB 14.4 MB/s eta 0:00:00\n","Using cached wheel-0.41.3-py3-none-any.whl (65 kB)\n","Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n","Installing collected packages: tensorboard-plugin-wit, libclang, keras, wheel, werkzeug, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, oauthlib, keras-preprocessing, h5py, grpcio, google-pasta, gast, cachetools, rsa, requests-oauthlib, pyasn1-modules, markdown, astunparse, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n","Successfully installed astunparse-1.6.3 cachetools-5.3.2 gast-0.4.0 google-auth-2.23.4 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.59.2 h5py-3.10.0 keras-2.10.0 keras-preprocessing-1.1.2 libclang-16.0.6 markdown-3.5.1 oauthlib-3.2.2 opt-einsum-3.3.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0 tensorflow-io-gcs-filesystem-0.31.0 werkzeug-3.0.1 wheel-0.41.3\n"]}],"source":["!pip install larq larq-zoo larq-compute-engine\n","!pip install tensorflow==2.10.0"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"1n_xcqlrsaMY"},"outputs":[],"source":["import tensorflow as tf\n","import larq as lq"]},{"cell_type":"markdown","metadata":{"id":"sZuhOF30TX-9"},"source":["We'll stick with simple 3-conv-layers CNN and MNIST dataset. This time, we'll implement it with `TensorFlow`\n","\n","First, we use `tf.keras.datasets.mnist.load_data()` to download a dataset. Then, we need to reshape images to `(numer_of_samples, image_width, image_height, number_of_channels)`. Finally, we normalize image values to be between -1 and 1."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"7cnaAnDMsovx"},"outputs":[],"source":["# Load the MNIST dataset\n","(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","\n","# Reshape the images to add a channel dimension (in MNIST, 1 because images are grayscale)\n","train_images = train_images.reshape((train_images.shape[0], 28, 28, 1))\n","test_images = test_images.reshape((test_images.shape[0], 28, 28, 1))\n","\n","# Normalize the images to [-1, 1]\n","train_images = train_images.astype('float32')\n","test_images = test_images.astype('float32')\n","\n","train_images = (train_images - 127.5) / 127.5\n","test_images = (test_images - 127.5) / 127.5"]},{"cell_type":"markdown","metadata":{"id":"h0qky25Y34Pn"},"source":["Now, we need to create our model. We'll use the same architecture as in `PyTorch`:\n","\n","```\n","nn.Conv2d(input_channels,32,3,padding=(1,1))\n","nn.BatchNorm2d(32)\n","nn.ReLU()\n","nn.MaxPool2d(2,2)\n","\n","nn.Conv2d(32,64,3,padding=(1,1))\n","nn.BatchNorm2d(64)\n","nn.ReLU()\n","nn.MaxPool2d(2,2)\n","\n","nn.Conv2d(64,128,3)\n","nn.BatchNorm2d(128)\n","nn.ReLU()\n","\n","nn.Flatten(),\n","nn.Linear(CNN_out_size, num_of_cls),\n","nn.Softmax(dim=1)\n","```\n","\n","In `TensorFlow` however we define model with `tf.keras.models.Sequential()` and then we add layers one by one with `model.add()`.\n","\n","Create model with the help of documentation (https://keras.io/api/layers/) and following tips. You're going to need following layers:\n","\n","- In `PyTorch` we created convolutional layers with `Conv2d(in_ch, out_ch, kernel_size, padding=(x,x))`.  For `TrensorFlow` we use `tf.keras.layers.Conv2D(out_ch, kernel_size, strides=(x,x), padding=\"same\")`.\n","- For `stride=(x,x)` and `padding=\"same\"` we get the same result as with `padding=(x,x)` in PyTorch. We don't have to specify number of input channels, but we have to specify `input_shape=(w,h,ch)` parameter for the first `Conv2D` layer.\n","- The activation function is not added as a additional layer, but as `activation=\"relu\"` parameter to `Conv2D`\n","- You'll need `tf.keras.layers.BatchNormalization(scale=False)`, `tf.keras.layers.MaxPooling2D((2, 2))`, `tf.keras.layers.Dense(output_size)` and `tf.keras.layers.Flatten()` layers.\n","- The last layer (softmax) in our model should be `tf.keras.layers.Activation(\"softmax\")`"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Ztc3uPjk3wm7"},"outputs":[],"source":["CNN = tf.keras.models.Sequential()\n","\n","# First convolutional block\n","CNN.add(tf.keras.layers.Conv2D(32, (3, 3), padding='same', input_shape=(28, 28, 1)))\n","CNN.add(tf.keras.layers.BatchNormalization())\n","CNN.add(tf.keras.layers.ReLU())\n","CNN.add(tf.keras.layers.MaxPooling2D((2, 2)))\n","\n","# Second convolutional block\n","CNN.add(tf.keras.layers.Conv2D(64, (3, 3), padding='same'))\n","CNN.add(tf.keras.layers.BatchNormalization())\n","CNN.add(tf.keras.layers.ReLU())\n","CNN.add(tf.keras.layers.MaxPooling2D((2, 2)))\n","\n","# Third convolutional block\n","CNN.add(tf.keras.layers.Conv2D(128, (3, 3), padding='valid'))\n","CNN.add(tf.keras.layers.BatchNormalization())\n","CNN.add(tf.keras.layers.ReLU())\n","\n","# Flatten and dense layers\n","CNN.add(tf.keras.layers.Flatten())\n","# The number of neurons in the linear layer (CNN_out_size) depends on the output of the last conv layer\n","# Let's calculate CNN_out_size\n","CNN_out_size = (28 // (2 * 2)) * (28 // (2 * 2)) * 128  # 28x28 is the input size and 2x2 pooling is applied twice\n","num_of_cls = 10  # number of classes for MNIST\n","CNN.add(tf.keras.layers.Dense(CNN_out_size))\n","CNN.add(tf.keras.layers.Dense(num_of_cls, activation='softmax'))"]},{"cell_type":"markdown","metadata":{"id":"MtG9rfTt--ng"},"source":["Hopefully, we have our network ready to go! You can use `CNN.summary()` function to see our ready network with it's shapes and parameters."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"tquhxcLk8kPe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 28, 28, 32)        320       \n","                                                                 \n"," batch_normalization (BatchN  (None, 28, 28, 32)       128       \n"," ormalization)                                                   \n","                                                                 \n"," re_lu (ReLU)                (None, 28, 28, 32)        0         \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 14, 14, 64)        18496     \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 14, 14, 64)       256       \n"," hNormalization)                                                 \n","                                                                 \n"," re_lu_1 (ReLU)              (None, 14, 14, 64)        0         \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 5, 5, 128)         73856     \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 5, 5, 128)        512       \n"," hNormalization)                                                 \n","                                                                 \n"," re_lu_2 (ReLU)              (None, 5, 5, 128)         0         \n","                                                                 \n"," flatten (Flatten)           (None, 3200)              0         \n","                                                                 \n"," dense (Dense)               (None, 6272)              20076672  \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                62730     \n","                                                                 \n","=================================================================\n","Total params: 20,232,970\n","Trainable params: 20,232,522\n","Non-trainable params: 448\n","_________________________________________________________________\n"]}],"source":["CNN.summary()"]},{"cell_type":"markdown","metadata":{"id":"R9qhy7nQ_VhE"},"source":["Now, let's train our network! After defining model layers and connections we need to compile it with `model.compile()` function. Google it and study its arguments. We should use `adam` optimizer, `[’accuracy’]`\n","metric and `tf.keras.losses.SparseCategoricalCrossentropy` loss.\n","\n","Then, we run training with `model.fit()` function. Google it and study its arguments. We'll train `CNN` with batch size of `64` for `5` epochs with `train_images` and `train_labels`. After training, evaluate network with `CNN.evaluate()`, where you pass only `test_images` and `test_labels`. Print the resulting accuracy. It should be around 99%!"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"1HlONKMo9Pbv"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\krzys\\Documents\\GitHub\\Wbudowane-systemy-AI\\.venv\\lib\\site-packages\\keras\\backend.py:5582: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n"]},{"name":"stdout","output_type":"stream","text":["938/938 [==============================] - 155s 164ms/step - loss: 0.5386 - accuracy: 0.9554\n","Epoch 2/5\n","938/938 [==============================] - 136s 145ms/step - loss: 0.1647 - accuracy: 0.9739\n","Epoch 3/5\n","938/938 [==============================] - 144s 154ms/step - loss: 0.1376 - accuracy: 0.9807\n","Epoch 4/5\n","180/938 [====>.........................] - ETA: 2:10 - loss: 0.0911 - accuracy: 0.9852"]}],"source":["# Compile the model\n","CNN.compile(\n","    optimizer='adam',\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=['accuracy']\n",")\n","\n","# Train the model\n","CNN.fit(\n","    train_images, \n","    train_labels, \n","    batch_size=64, \n","    epochs=5\n",")\n","\n","# Evaluate the model\n","test_loss, test_acc = CNN.evaluate(test_images, test_labels)\n","\n","# Print the accuracy\n","print(f\"Test accuracy: {test_acc*100:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"lpnfcPAcAOxY"},"source":["We got familiar with `TensorFlow` basics and we can carry on with BNNs.\n","\n","Create new model with `tf.keras.models.Sequential()` and call it `BNN`. Use `model.add()` function to add all nessesery layers to this model. However, you'll need to replace all `Conv2D` and `Dense` layers with their binary counterparts. Use `lq.layers.QuantConv2D()` instead of `tf.keras.layers.Conv2D()` and `lq.layers.QuantDense()` instead of `tf.keras.layers.Dense()`.\n","\n","They use more or less the same parameters, but additionally, you have to update thair quantizers. In BNNs, the quantization function\n","$$\n","q(x) = \\begin{cases}\n","    -1 & x < 0 \\\\\\\n","    1 & x \\geq 0\n","\\end{cases}\n","$$\n","is used in the forward pass to binarize the activations and the latent full precision weights. The gradient of this function is zero almost everywhere which prevents the model from learning.\n","\n","To be able to train the model the gradient is instead estimated using the Straight-Through Estimator (STE):\n","$$\n","\\frac{\\partial q(x)}{\\partial x} = \\begin{cases}\n","    1 & \\left|x\\right| \\leq 1 \\\\\\\n","    0 & \\left|x\\right| > 1\n","\\end{cases}\n","$$\n","\n","In Larq this can be done by using `input_quantizer=\"ste_sign\"` and `kernel_quantizer=\"ste_sign\"`.\n","Additionally, the latent full precision weights are clipped to -1 and 1 using `kernel_constraint=\"weight_clip\"`.\n","\n","For the first Conv2d layer add parameters `kernel_quantizer=\"ste_sign\"` and `kernel_constraint=\"weight_clip\"`. For the next ones (both `QuantConv2D` and `QuantDense`) use those two and `input_quantizer=\"ste_sign\"`. This is why we don't quantize the inputs to the first convolutional layer (as is common for BNN training). All other layers should stay the same as in `CNN`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DDQd9U-hoPiT"},"outputs":[],"source":["BNN = tf.keras.models.Sequential()\n","\n","# First binary convolutional layer\n","# We only quantize the kernel and not the input for the first layer\n","BNN.add(lq.layers.QuantConv2D(\n","    32, (3, 3), \n","    padding='same', \n","    input_shape=(28, 28, 1),\n","    kernel_quantizer=\"ste_sign\",\n","    kernel_constraint=\"weight_clip\"\n","))\n","BNN.add(tf.keras.layers.BatchNormalization())\n","BNN.add(tf.keras.layers.ReLU())\n","BNN.add(tf.keras.layers.MaxPooling2D((2, 2)))\n","\n","# Second binary convolutional layer\n","BNN.add(lq.layers.QuantConv2D(\n","    64, (3, 3), \n","    padding='same', \n","    input_quantizer=\"ste_sign\",\n","    kernel_quantizer=\"ste_sign\",\n","    kernel_constraint=\"weight_clip\"\n","))\n","BNN.add(tf.keras.layers.BatchNormalization())\n","BNN.add(tf.keras.layers.ReLU())\n","BNN.add(tf.keras.layers.MaxPooling2D((2, 2)))\n","\n","# Third binary convolutional layer\n","BNN.add(lq.layers.QuantConv2D(\n","    128, (3, 3),\n","    padding='valid', \n","    input_quantizer=\"ste_sign\",\n","    kernel_quantizer=\"ste_sign\",\n","    kernel_constraint=\"weight_clip\"\n","))\n","BNN.add(tf.keras.layers.BatchNormalization())\n","BNN.add(tf.keras.layers.ReLU())\n","\n","# Flatten the outputs for the dense layers\n","BNN.add(tf.keras.layers.Flatten())\n","\n","# First binary dense layer\n","BNN.add(lq.layers.QuantDense(\n","    CNN_out_size, \n","    input_quantizer=\"ste_sign\",\n","    kernel_quantizer=\"ste_sign\",\n","    kernel_constraint=\"weight_clip\"\n","))\n","\n","# Second binary dense (output) layer\n","BNN.add(lq.layers.QuantDense(\n","    num_of_cls, \n","    input_quantizer=\"ste_sign\",\n","    kernel_quantizer=\"ste_sign\",\n","    kernel_constraint=\"weight_clip\",\n","    activation='softmax'\n","))\n"]},{"cell_type":"markdown","metadata":{"id":"KtqxtCnbDWnt"},"source":["You can use `lq.models.summary(BNN)` function to see our ready network with it's shapes and parameters. Study the quantization summary at the bottom."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MFadMgRhkx6t"},"outputs":[],"source":["lq.models.summary(BNN)"]},{"cell_type":"markdown","metadata":{"id":"tE6yPpFmDlXO"},"source":["Now, compile, fit for 5 epochs and evaluate your `BNN` (the same way as before). Print the result. Is such a network enough for such a task?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VUrOsuemk3ja"},"outputs":[],"source":["# Compile the BNN model\n","BNN.compile(\n","    optimizer='adam',\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=['accuracy']\n",")\n","\n","# Fit the BNN model\n","BNN.fit(\n","    train_images, \n","    train_labels, \n","    batch_size=64, \n","    epochs=5\n",")\n","\n","# Evaluate the BNN model\n","test_loss, test_acc = BNN.evaluate(test_images, test_labels)\n","\n","# Print the accuracy\n","print(f\"Test accuracy: {test_acc*100:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"F7stUeBGEBwQ"},"source":["**Extenction exercise**\n","\n","Deploy this network for ARM-based board (like RaspberryPI) or Android app with Larq Compute Engine. Benchmark it's performance.\n","\n","Find out more here: https://docs.larq.dev/compute-engine/"]},{"cell_type":"markdown","metadata":{"id":"AGgn0Ay8Ec3N"},"source":["# Spiking Neural Networks\n","\n","Spiking Neural Networks (SNNs) are a type of artificial neural network that is designed to simulate the behavior of neurons in the brain. In a traditional artificial neural network (ANN), the neurons are modeled as having a continuous activation value that changes over time, whereas in an SNN, the neurons are modeled as having discrete \"spikes\" of activation that occur at specific\n","points in time.\n","\n","SNNs are inspired by the way the neurons in the brain work. In the brain, neurons communicate with each other by sending electrical pulses, or spikes, along their axons. These spikes propagate to the dendrites of other neurons and, if the total input to a neuron exceeds a certain threshold, the neuron will generate a spike in response. SNNs use a similar concept: the neurons\n","in an SNN have a threshold, and if the total input to a neuron exceeds that threshold, the neuron will generate a spike.\n","\n","SNNs have several advantages over traditional ANNs. One of the main advantages is that SNNs are more energy efficient, because they only communicate when they need to, instead of continuously sending signals. Additionally, SNNs can be more robust to noise and other disturbances, because they can use the timing of spikes to communicate information. Currently SNNs are not as popular as traditional neural networks (ANNs) in industry and research, because of their higher complexity, specialized hardware requirements and less mature toolkits.\n","\n","SNN takes a set of spikes as input and produces a set of spikes as output. The general idea is:\n","- Each neuron has a value that is equivalent to the electrical potential of biological neurons at any given time.\n","- The value of a neuron can change according to its mathematical model; for example, if a neuron gets a spike from an upstream neuron, its value may rise or fall.\n","- If a neuron’s value surpasses a certain threshold, the neuron will send a single impulse to each downstream neuron connected to the first one, and the neuron’s value will immediately drop below its average.\n","- As a result, the neuron will go through a refractory period similar to that of a biological neuron. The neuron’s value will gradually return to its average over time.\n","\n","We'll use `SnnTorch` framework. We'll not dive deep into this idea, we'll just get familiar with the basics. SNNs are very tricky to train, and their research field is still quite new. Moreover, they need state-of-the-art neuromorphic computing platforms like Brainchip's Akida or Intel's Loihi.\n","\n","Read more here: https://snntorch.readthedocs.io/en/latest/index.html\n","\n","First, install and import libraries:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_pCCVe9SUUUe"},"outputs":[],"source":["!pip install tonic --quiet\n","!pip install snntorch --quiet\n","import tonic\n","import matplotlib.pyplot as plt\n","import snntorch.spikeplot as splt\n","from IPython.display import HTML\n","import snntorch as snn\n","from snntorch import surrogate\n","from snntorch import functional as SF\n","from snntorch import spikeplot as splt\n","from snntorch import utils\n","import torch.nn as nn\n","import torch\n","from torch.utils.data import DataLoader"]},{"cell_type":"markdown","metadata":{"id":"bB9P8z9iuWZo"},"source":["Let's start our journey with the input spikes. Neuromorphic NNs work best with... neuromorphic sensors like Event Cameras.\n","\n","An event camera (also known as Dynamic Vision Sensor – DVS) is a neuromorphic sensor that takes its inspiration from the human eye. Unlike classical cameras, which record the brightness (colour) level for a given pixel every specified time interval (frame per second parameter), a DVS records brightness changes independently (asynchronously) for individual pixels. Consequently, the data captured by the camera does not depend on the clock but the dynamics of the scene. As a result, a stream of events is available on the output, where each is described by 4 values:\n","* x & y co-ordinates correspond to an address in a $34 \\times 34$ grid.\n","\n","* The timestamp of the event is recorded in microseconds.\n","\n","* The polarity refers to whether an on-spike (+1) or an off-spike (-1) occured; i.e., an increase in brightness or a decrease in brightness.\n","\n","We can use an output of DVS as input to SNN (with some basics transformation, but it does not metter for now). In this task we'll use NMNIST dataset, with is just MNIST dataset recorded with event camera.\n","\n","Use the following code to download a NMNIST dataset and visualise it. What we can see is the events captured for each pixel (x, y) in time. The colour means a postive or negative polarity."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xhjyeQyNUhMf"},"outputs":[],"source":["nmnist = tonic.datasets.NMNIST(save_to='./data', train=False)\n","\n","events, label = nmnist[0]\n","transform = tonic.transforms.ToFrame(\n","    sensor_size=nmnist.sensor_size,\n","    time_window=10000,\n",")\n","frames = transform(events)\n","animation = tonic.utils.plot_animation(frames)\n","\n","# Display the animation inline in a Jupyter notebook\n","from IPython.display import HTML\n","HTML(animation.to_jshtml())"]},{"cell_type":"markdown","metadata":{"id":"dw_IBE0wxa8a"},"source":["During this task, we'll not train the network. We'll use already pretrained model (from UPEL). Use following line to upload `snn.pth` file, and then run cells with the SNN definition and loading of the weights."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6GIs145EmRXt"},"outputs":[],"source":["#from google.colab import files\n","#files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LCOVwO9Nig0-"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# neuron and simulation parameters\n","spike_grad = surrogate.atan()\n","beta = 0.5\n","\n","#  Initialize Network\n","net = nn.Sequential(nn.Conv2d(2, 12, 5),\n","                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n","                    nn.MaxPool2d(2),\n","                    nn.Conv2d(12, 32, 5),\n","                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n","                    nn.MaxPool2d(2),\n","                    nn.Flatten(),\n","                    nn.Linear(32*5*5, 10),\n","                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n","                    )\n","\n","testset = tonic.datasets.NMNIST(save_to='./tmp/data', transform=transform, train=False)\n","testloader = DataLoader(testset, batch_size=1, collate_fn=tonic.collation.PadTensors(batch_first=False))\n","net.load_state_dict(torch.load(\"snn.pth\"))"]},{"cell_type":"markdown","metadata":{"id":"1D04sddKyB1c"},"source":["Now, let's study the output of the SNN. As we already now, the output is spiking as well. We remeber, that for CNNs the output for MNIST classification is a 10-values vector, where each value refers to the probability, that the object belongs to each class.\n","\n","For SNNs we have a 10-values vector as well, but each can only be active or inactive (0 or 1). So how can we tell, to which class does the object belongs? We just count the number of spikes registered for each class!\n","\n","In the following code, we take a sample from dataset, we perform the forward-pass and visualize the output. Study it!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MHzSnPmIm7ew"},"outputs":[],"source":["def forward_pass(net, data):\n","  spk_rec = []\n","  utils.reset(net)  # resets hidden states for all LIF neurons in net\n","\n","  for step in range(data.size(0)):  # data.size(0) = number of time steps\n","      spk_out, mem_out = net(data[step])\n","      spk_rec.append(spk_out)\n","\n","  return torch.stack(spk_rec)\n","\n","event_tensor, target = next(iter(testloader))\n","spk_rec = forward_pass(net, event_tensor)\n","idx = 0\n","fig, ax = plt.subplots(facecolor='w', figsize=(12, 7))\n","labels=['0', '1', '2', '3', '4', '5', '6', '7', '8','9']\n","print(f\"The target label is: {target}\")\n","anim = splt.spike_count(spk_rec[:, idx].detach().cpu(), fig, ax, labels=labels,\n","                        animate=True, interpolate=5)\n","HTML(anim.to_html5_video())"]},{"cell_type":"markdown","metadata":{"id":"yhfoLSVIzGMV"},"source":["That's it! There were not much for you to do in this part of the class. The teacher will ask you few questions about SNNs just to verify, that you got the general idea :)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
