{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying ANN models on USB accelerators\n",
    "\n",
    "One of the most important issues with big ANN models is the time it takes to compute the result for a sigle input, which may sometimes be quite long. Another issue is that high-advanced models, often consisting of billions of parameters, require a lot of energy to run.\n",
    "\n",
    "It is especially troubling when we would like to use such models for specific applicatons e.g. vision system, where we would like to achive high effectivness of the model (like to detect as much objects as posibble with high probability) in short period of time (for standard cameras image acquisition frequency is 60Hz which would mean that the model must perform inference on input image in less than 16 ms). Additionaly, we are often limited by the hardware (be it small CPUs, low RAM memory or limited power supply).\n",
    "\n",
    "For that reason some companies developed products with purpuse to speed up ANN models while trying to minimise the consumed energy. In todays laboratory we will use USB accelerators to implement an application that will take image from webcamera and use MobileNetV2 model to classify the objects in the image. The application will be quite simple and the only differene will be the core hardware on which the model will be executed, that is:\n",
    "\n",
    "- CPU (on local machine)\n",
    "- Intel Neural Compute Stick 2 (VPU)\n",
    "- Google Coral USB Accelerator (TPU)\n",
    "\n",
    "In this notebook we will focus on installing frameworks for deploying model on devices and to verifing the application (using local CPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework installation\n",
    "\n",
    "### Python libraries\n",
    "\n",
    "When working on various applications with different devices, it is a good practice to not dump all necessary libraries into one place. That is why we will use Python virtual environment, as not to meddle with the global environment. This way if we mess something up with application dependencies, the main interpreter will remain intact. Remember to use Python version not lower that 3.7 and not higher than 3.10. To create Python virtual environment execute the following in your working directory (`.venv_ncs_cua` is an example name, you can change it if you want to):\n",
    "\n",
    "```\n",
    "python3 -m venv .venv_ncs_cua\n",
    "source .venv_ncs_cua/bin/activate\n",
    "```\n",
    "\n",
    "By executing `source .venv_ncs_cua/bin/activate` you activate the virtual environment, meaning every Python script executed in current cmd will use the Python interpreter from it.\n",
    "\n",
    "Next, install following libraries using pip:\n",
    "\n",
    "```\n",
    "pip install jupyter opencv-python tensorflow==2.9.3 openvino==2022.3.1 openvino-dev[tensorflow2]==2022.3.1 --extra-index-url https://google-coral.github.io/py-repo/ pycoral~=2.0\n",
    "```\n",
    "\n",
    "Due to discontinuation of support for Intel NCS2, we cannot use the lates OpenVINO version (the one you used on previous laboratories) and must work with older ones. This also means that some other libraries that OV depends on, like TensorFlow or Numpy, will be installed in older version too (and this is why virtual environmet should be used).\n",
    "\n",
    "Lastly check if everything is in place:\n",
    "\n",
    "```\n",
    "pip check\n",
    "```\n",
    "\n",
    "Please, verify also if you installed OpenVINO properly be executing below command:\n",
    "\n",
    "```\n",
    "mo --help\n",
    "```\n",
    "\n",
    "### Intel NCS2\n",
    "\n",
    "While OpenVINO library would be enough to convert our model and run it on local CPU, if we want to deploy it on NCS, some additional configurations are necessary. We will follow [this](https://www.intel.com/content/www/us/en/developer/articles/guide/get-started-with-neural-compute-stick.html) guide. First, download [OpenVINO Runtime](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/download.html?VERSION=v_2022_3_1&ENVIRONMENT=RUNTIME&OP_SYSTEM=LINUX&DISTRIBUTION=ARCHIVE) in version 2022.3.1 for Linux systems directly from [archives](https://storage.openvinotoolkit.org/repositories/openvino/packages/2022.3.1/linux) (it will provide us with all we need in the easiest way). Below you will find steps to do so using command console:\n",
    "\n",
    "```\n",
    "curl -L https://storage.openvinotoolkit.org/repositories/openvino/packages/2022.3.1/linux/l_openvino_toolkit_ubuntu20_2022.3.1.9227.cf2c7da5689_x86_64.tgz --output openvino_2022.3.1.tgz\n",
    "tar -xf openvino_2022.3.1.tgz\n",
    "mv l_openvino_toolkit_ubuntu20_2022.3.1.9227.cf2c7da5689_x86_64 openvino_2022.3.1\n",
    "```\n",
    "\n",
    "Next install OpenVINO Runtime:\n",
    "\n",
    "```\n",
    "cd openvino_2022.3.1\n",
    "sudo -E ./install_dependencies/install_openvino_dependencies.sh\n",
    "sudo ./install_dependencies/install_NCS_udev_rules.sh\n",
    "cd ..\n",
    "```\n",
    "\n",
    "Lastly, before you try to deploy your model on NCS2, remember to update path in your cmd with OpenVINO variables (we will do it when the times come):\n",
    "\n",
    "```\n",
    "source openvino_2022.3.1/setupvars.sh\n",
    "```\n",
    "\n",
    "### Google Coral USB Accelerator\n",
    "\n",
    "Similary to NCS, it is also required to install additional libraries for Google Coral to connect with our application. We will follow [this](https://coral.ai/docs/accelerator/get-started) guide. Below you will find steps to install necessary libraries using command console:\n",
    "\n",
    "```\n",
    "echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
    "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
    "sudo apt-get update\n",
    "sudo apt-get install libedgetpu1-std\n",
    "```\n",
    "\n",
    "Additionally, we will need [Edge TPU compiler](https://coral.ai/docs/edgetpu/compiler/) that will allow us to convert prepared model to be more TPU-friendly which will increase it runtime speed. Below you will find steps to install it in comand console:\n",
    "\n",
    "```\n",
    "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
    "echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
    "sudo apt-get update\n",
    "sudo apt-get install edgetpu-compiler\n",
    "```\n",
    "\n",
    "Verify installation:\n",
    "\n",
    "```\n",
    "edgetpu_compiler --help\n",
    "```\n",
    "\n",
    "In case the copiler does not work, Google provides online tool in a form of [Colab Notebook](https://colab.research.google.com/github/google-coral/tutorials/blob/master/compile_for_edgetpu.ipynb). There you will find set of instructions how to compile your model using that notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model preparation and application verification\n",
    "\n",
    "Now everything is ready and we can start implementig our application. In this notebook you don't have to use earlier created virtual environmet, but if you would like to, you have to change the Python interpreter to the one inside your venv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from typing import Any\n",
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "\n",
    "from usb_accelerator_utils import draw_classification_results, run_program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the application we will use [MobileNetV2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v2/MobileNetV2) model, but you can try and use [other](https://www.tensorflow.org/api_docs/python/tf/keras/applications) ones as well. The imported model has already pre-trainded weights on [ImageNet](https://www.image-net.org/) dataset.\n",
    "\n",
    "```\n",
    "! Important !\n",
    "```\n",
    "\n",
    "As we will use model pretrained and imported from TensorFlow, we need to verify what is the shape of the input data and waht is the range of the values (whether input is normalized). As for MobileNetV2 from Keras, input data range must be in `[-1, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO: Load from the TF library MobileNetV2 model with pre-trained weights\n",
    "MobNet = ...\n",
    "\n",
    "print('Input  shape:', MobNet.input_shape)\n",
    "print('Output shape:', MobNet.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO: Set the shape of input data i.e. size of input image (it will be later used for input scaling)\n",
    "INPUT_IMAGE_WIDTH  = ...\n",
    "INPUT_IMAGE_HEIGHT = ...\n",
    "print(f'Input  shape: ({INPUT_IMAGE_HEIGHT}, {INPUT_IMAGE_WIDTH})')\n",
    "\n",
    "# @TODO: Load label names for MobilNet classes, you can find example files here: https://coral.ai/models/all/. Does labels' number equals model output shape?\n",
    "LABELS = ...\n",
    "print('Labels shape:', np.shape(LABELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of our application is different basing on hardware we use. Below function is used for executing TensorFlow model on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classification(exec_net: Any, img: np.ndarray, max_classes: int=1, min_score: float=0.0) -> float:\n",
    "  \"\"\"Perforn one inference and draw results.\n",
    "\n",
    "  Args:\n",
    "    exec_net (Any): ANN model that performs classification.\n",
    "    img (np.ndarray): Input image.\n",
    "    max_classes (int, optional): Max number of best detections to diaplay. Defaults to 1.\n",
    "    min_score (float, optional): Min score a detection should have to be displayed. Defaults to 0.0.\n",
    "\n",
    "  Returns:\n",
    "  float: Inference time in seconds.\n",
    "\"\"\"\n",
    "  # @TODO: Prepare input image - modify image shape to fit into model input\n",
    "  #        Hint1: How many dimension the input image need to have if only one image is processed at the time?\n",
    "  #        Hint2: Use imported from TF function `preprocess_input` prepare image for MobileNet\n",
    "  conv_img = ...\n",
    "  \n",
    "  t0 = time.time()\n",
    "  \n",
    "  # @TODO: Perform one inference on prepared data\n",
    "  result = ...\n",
    "  \n",
    "  elapsed = time.time() - t0\n",
    "  \n",
    "  # @TODO: Extract classification scores, make it as a one-dimensional array\n",
    "  scores = ...\n",
    "  \n",
    "  class_idxs = np.arange(start=0, stop=scores.shape[0], step=1, dtype=int)\n",
    "  draw_classification_results(\n",
    "    img=img,\n",
    "    class_idxs=class_idxs,\n",
    "    scores=scores,\n",
    "    labels=LABELS,\n",
    "    max_classes=max_classes,\n",
    "    min_score=min_score)\n",
    "  return elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally run the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_program(\n",
    "  exec_net=MobNet,\n",
    "  c_func=run_classification,\n",
    "  camera_idx=0,\n",
    "  max_disp=3,\n",
    "  min_score=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step is to export our model, so that it can be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO: Provide path to place where model will be saved\n",
    "model_tf_dir = Path(...)\n",
    "\n",
    "model_tf_dir.mkdir(parents=True, exist_ok=True)\n",
    "MobNet.save(str(model_tf_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
