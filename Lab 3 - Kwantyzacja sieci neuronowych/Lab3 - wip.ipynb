{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHGC0EFt3aZF"
   },
   "source": [
    "# Neural networks quantization\n",
    "\n",
    "Today we will deal with neural networks quantization!\n",
    "\n",
    "Remember that our goal is to reduce network size while keeping the accuracy high!\n",
    "\n",
    "For this purpose we will use Intel's OpenVino and Neural Network Compression Framework (NNCF). Be aware, that there are other frameworks to choose from: buildin PyTorch quantization, Brevitas from Xilinx, TensorRT and others.\n",
    "\n",
    "Use this link for OpenVino reference and documentation: https://docs.openvino.ai/2023.0/home.html\n",
    "\n",
    "First, install and import nessessery libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bNKDyp4Ssmcw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openvino\n",
      "  Downloading openvino-2023.1.0-12185-cp39-cp39-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openvino) (1.23.5)\n",
      "Collecting openvino-telemetry>=2023.1.0 (from openvino)\n",
      "  Downloading openvino_telemetry-2023.2.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading openvino-2023.1.0-12185-cp39-cp39-win_amd64.whl (28.7 MB)\n",
      "   ---------------------------------------- 0.0/28.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/28.7 MB 3.2 MB/s eta 0:00:09\n",
      "    --------------------------------------- 0.6/28.7 MB 7.6 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 1.9/28.7 MB 15.4 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 4.1/28.7 MB 23.7 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 6.3/28.7 MB 30.8 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 8.9/28.7 MB 33.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 11.5/28.7 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 11.5/28.7 MB 59.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 12.6/28.7 MB 40.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 12.6/28.7 MB 40.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 12.6/28.7 MB 40.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 12.6/28.7 MB 40.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 12.6/28.7 MB 40.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 12.6/28.7 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 13.0/28.7 MB 19.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 15.7/28.7 MB 19.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 16.4/28.7 MB 17.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 17.7/28.7 MB 17.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 18.9/28.7 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 20.2/28.7 MB 16.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 21.2/28.7 MB 14.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 22.4/28.7 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 23.9/28.7 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 25.2/28.7 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 26.5/28.7 MB 27.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 27.8/28.7 MB 27.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  28.6/28.7 MB 27.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  28.6/28.7 MB 27.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.7/28.7 MB 23.4 MB/s eta 0:00:00\n",
      "Downloading openvino_telemetry-2023.2.1-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: openvino-telemetry, openvino\n",
      "Successfully installed openvino-2023.1.0 openvino-telemetry-2023.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nncf\n",
      "  Downloading nncf-2.6.0-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: jsonschema>=3.2.0 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nncf) (4.17.3)\n",
      "Collecting jstyleson>=0.0.2 (from nncf)\n",
      "  Downloading jstyleson-0.0.2.tar.gz (2.0 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting natsort>=7.1.0 (from nncf)\n",
      "  Downloading natsort-8.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting networkx<=2.8.2,>=2.6 (from nncf)\n",
      "  Downloading networkx-2.8.2-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.1/2.0 MB 3.6 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 0.6/2.0 MB 7.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 2.0/2.0 MB 15.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.0/2.0 MB 14.2 MB/s eta 0:00:00\n",
      "Collecting ninja<1.11,>=1.10.0.post2 (from nncf)\n",
      "  Downloading ninja-1.10.2.4-py2.py3-none-win_amd64.whl (293 kB)\n",
      "     ---------------------------------------- 0.0/293.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 293.9/293.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: numpy<1.25,>=1.19.1 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nncf) (1.23.5)\n",
      "Requirement already satisfied: openvino-telemetry>=2023.1.1 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nncf) (2023.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\krzys\\appdata\\roaming\\python\\python39\\site-packages (from nncf) (23.1)\n",
      "Requirement already satisfied: pandas<2.1,>=1.1.5 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nncf) (2.0.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\krzys\\appdata\\roaming\\python\\python39\\site-packages (from nncf) (5.9.5)\n",
      "Collecting pydot>=1.4.1 (from nncf)\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting pymoo>=0.6.0.1 (from nncf)\n",
      "  Downloading pymoo-0.6.0.1-cp39-cp39-win_amd64.whl (711 kB)\n",
      "     ---------------------------------------- 0.0/711.7 kB ? eta -:--:--\n",
      "     ------------------------------------- 711.7/711.7 kB 43.8 MB/s eta 0:00:00\n",
      "Collecting pyparsing<3.0 (from nncf)\n",
      "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "     ---------------------------------------- 0.0/67.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 67.8/67.8 kB 3.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn>=0.24.0 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nncf) (1.2.2)\n",
      "Requirement already satisfied: scipy<1.11,>=1.3.2 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nncf) (1.10.1)\n",
      "Collecting texttable>=1.6.3 (from nncf)\n",
      "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.54.1 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nncf) (4.65.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jsonschema>=3.2.0->nncf) (23.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jsonschema>=3.2.0->nncf) (0.19.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\krzys\\appdata\\roaming\\python\\python39\\site-packages (from pandas<2.1,>=1.1.5->nncf) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas<2.1,>=1.1.5->nncf) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas<2.1,>=1.1.5->nncf) (2023.3)\n",
      "Requirement already satisfied: matplotlib>=3 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pymoo>=0.6.0.1->nncf) (3.7.1)\n",
      "Collecting autograd>=1.4 (from pymoo>=0.6.0.1->nncf)\n",
      "  Downloading autograd-1.6.2-py3-none-any.whl.metadata (706 bytes)\n",
      "Collecting cma==3.2.2 (from pymoo>=0.6.0.1->nncf)\n",
      "  Downloading cma-3.2.2-py2.py3-none-any.whl (249 kB)\n",
      "     ---------------------------------------- 0.0/249.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 249.1/249.1 kB ? eta 0:00:00\n",
      "Collecting alive-progress (from pymoo>=0.6.0.1->nncf)\n",
      "  Downloading alive_progress-3.1.4-py3-none-any.whl.metadata (68 kB)\n",
      "     ---------------------------------------- 0.0/68.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 68.4/68.4 kB ? eta 0:00:00\n",
      "Collecting dill (from pymoo>=0.6.0.1->nncf)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting Deprecated (from pymoo>=0.6.0.1->nncf)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn>=0.24.0->nncf) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn>=0.24.0->nncf) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\krzys\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.54.1->nncf) (0.4.6)\n",
      "Collecting future>=0.15.2 (from autograd>=1.4->pymoo>=0.6.0.1->nncf)\n",
      "  Downloading future-0.18.3.tar.gz (840 kB)\n",
      "     ---------------------------------------- 0.0/840.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 840.9/840.9 kB ? eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf) (9.5.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf) (5.12.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<2.1,>=1.1.5->nncf) (1.16.0)\n",
      "Collecting about-time==4.2.1 (from alive-progress->pymoo>=0.6.0.1->nncf)\n",
      "  Downloading about_time-4.2.1-py3-none-any.whl (13 kB)\n",
      "Collecting grapheme==0.6.0 (from alive-progress->pymoo>=0.6.0.1->nncf)\n",
      "  Downloading grapheme-0.6.0.tar.gz (207 kB)\n",
      "     ---------------------------------------- 0.0/207.3 kB ? eta -:--:--\n",
      "     ------------------------------------- 207.3/207.3 kB 12.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from Deprecated->pymoo>=0.6.0.1->nncf) (1.14.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=3->pymoo>=0.6.0.1->nncf) (3.15.0)\n",
      "Downloading nncf-2.6.0-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 70.6 MB/s eta 0:00:00\n",
      "Downloading natsort-8.4.0-py3-none-any.whl (38 kB)\n",
      "Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading autograd-1.6.2-py3-none-any.whl (49 kB)\n",
      "   ---------------------------------------- 0.0/49.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 49.3/49.3 kB 2.6 MB/s eta 0:00:00\n",
      "Downloading alive_progress-3.1.4-py3-none-any.whl (75 kB)\n",
      "   ---------------------------------------- 0.0/75.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 75.9/75.9 kB ? eta 0:00:00\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 115.3/115.3 kB 6.6 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: jstyleson, grapheme, future\n",
      "  Building wheel for jstyleson (setup.py): started\n",
      "  Building wheel for jstyleson (setup.py): finished with status 'done'\n",
      "  Created wheel for jstyleson: filename=jstyleson-0.0.2-py3-none-any.whl size=2411 sha256=22f37af8bc1b307bc01fd7137c214004cdebc7f6c27671928d95b72b210f540c\n",
      "  Stored in directory: c:\\users\\krzys\\appdata\\local\\pip\\cache\\wheels\\d4\\2a\\06\\11202ea86be0f51f34e9411d691e25b991d188d93ab4d3e551\n",
      "  Building wheel for grapheme (setup.py): started\n",
      "  Building wheel for grapheme (setup.py): finished with status 'done'\n",
      "  Created wheel for grapheme: filename=grapheme-0.6.0-py3-none-any.whl size=210134 sha256=be39c14629513f84de808a84f5c2c7bc755058324d81dbfe6d8561c893171634\n",
      "  Stored in directory: c:\\users\\krzys\\appdata\\local\\pip\\cache\\wheels\\91\\a8\\bc\\5b55a10f763ee2c066ff2d94058d7dd64d570fc45adc027b3d\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492069 sha256=95eacde99571836102a9c5980d1b2be54e8aaf6188671a3ca1e9aa47edac3099\n",
      "  Stored in directory: c:\\users\\krzys\\appdata\\local\\pip\\cache\\wheels\\bf\\5d\\6a\\2e53874f7ec4e2bede522385439531fafec8fafe005b5c3d1b\n",
      "Successfully built jstyleson grapheme future\n",
      "Installing collected packages: texttable, ninja, jstyleson, grapheme, pyparsing, networkx, natsort, future, dill, Deprecated, cma, about-time, pydot, autograd, alive-progress, pymoo, nncf\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.0.9\n",
      "    Uninstalling pyparsing-3.0.9:\n",
      "      Successfully uninstalled pyparsing-3.0.9\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.1\n",
      "    Uninstalling networkx-3.1:\n",
      "      Successfully uninstalled networkx-3.1\n",
      "Successfully installed Deprecated-1.2.14 about-time-4.2.1 alive-progress-3.1.4 autograd-1.6.2 cma-3.2.2 dill-0.3.7 future-0.18.3 grapheme-0.6.0 jstyleson-0.0.2 natsort-8.4.0 networkx-2.8.2 ninja-1.10.2.4 nncf-2.6.0 pydot-1.4.2 pymoo-0.6.0.1 pyparsing-2.4.7 texttable-1.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\krzys\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install openvino\n",
    "!pip3 install nncf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1n_xcqlrsaMY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, openvino\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nncf\n",
    "import openvino as ov\n",
    "import time\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "from nncf import NNCFConfig\n",
    "from nncf.torch import create_compressed_model, register_default_init_args\n",
    "from openvino.runtime.ie_api import CompiledModel\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, RandomRotation\n",
    "from typing import Union, List, Tuple, Any\n",
    "from abc import ABC, abstractmethod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEGYhjeZ453d"
   },
   "source": [
    "Let's start with...\n",
    "\n",
    "##Quantizing Models Post-training\n",
    "\n",
    "Post-training model optimization is the process of applying special methods that transform the model into a more hardware-friendly representation without retraining or fine-tuning. The most popular and widely-spread method here is 8-bit post-training quantization because it is:\n",
    "\n",
    "- It is easy-to-use.\n",
    "- It does not hurt accuracy a lot.\n",
    "- It provides significant performance improvement.\n",
    "- It suites many hardware available in stock since most of them support 8-bit computation natively.\n",
    "\n",
    "8-bit integer quantization lowers the precision of weights and activations to 8 bits, which leads to significant reduction in the model footprint and significant improvements in inference speed.\n",
    "\n",
    "Source: https://docs.openvino.ai/2023.0/ptq_introduction.html\n",
    "\n",
    "So first! We need a model to quantize.\n",
    "Reuse the CNN model from Laboratory 1 (along with training loops, metics, optimazers and loss function).\n",
    "\n",
    "Train it for 5 epochs with MNIST dataset. You should get around ~90% accuracy.\n",
    "\n",
    "Name the final trained model `CNN_MNIST`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7cnaAnDMsovx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 33545951.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 1902644.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 12046261.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 4116363.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Epoch 1/5 => Train loss: 0.1332, Train accuracy: 95.91% | Test loss: 0.0468, Test accuracy: 98.47%\n",
      "Epoch 2/5 => Train loss: 0.0413, Train accuracy: 98.72% | Test loss: 0.0351, Test accuracy: 98.97%\n",
      "Epoch 3/5 => Train loss: 0.0295, Train accuracy: 99.06% | Test loss: 0.0319, Test accuracy: 98.94%\n",
      "Epoch 4/5 => Train loss: 0.0214, Train accuracy: 99.28% | Test loss: 0.0270, Test accuracy: 99.18%\n",
      "Epoch 5/5 => Train loss: 0.0161, Train accuracy: 99.49% | Test loss: 0.0267, Test accuracy: 99.21%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(64*4*4, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 64*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Set device\n",
    "torch_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Prepare datasets and dataloaders\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Model, metric, loss, and optimizer\n",
    "CNN_MNIST = CNN().to(torch_device)\n",
    "metric = nn.CrossEntropyLoss()\n",
    "loss_fcn = nn.CrossEntropyLoss().to(torch_device)\n",
    "optimizer = Adam(CNN_MNIST.parameters(), lr=0.001)\n",
    "\n",
    "# Train and test loops\n",
    "def train_test_pass(model, data_loader, loss_function, opt=None):\n",
    "    is_train = model.training\n",
    "    total_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    \n",
    "    for inputs, targets in data_loader:\n",
    "        inputs, targets = inputs.to(torch_device), targets.to(torch_device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        if is_train:\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_preds += (preds == targets).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    accuracy = 100.0 * correct_preds / len(data_loader.dataset)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def training(model, train_dl, test_dl, loss_fcn, optimizer, epochs=5):\n",
    "    history = {\"train_loss\": [], \"train_accuracy\": [], \"test_loss\": [], \"test_accuracy\": []}\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_accuracy = train_test_pass(model, train_dl, loss_fcn, optimizer)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_accuracy\"].append(train_accuracy)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss, test_accuracy = train_test_pass(model, test_dl, loss_fcn)\n",
    "            history[\"test_loss\"].append(test_loss)\n",
    "            history[\"test_accuracy\"].append(test_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} => \"\n",
    "              f\"Train loss: {train_loss:.4f}, Train accuracy: {train_accuracy:.2f}% | \"\n",
    "              f\"Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.2f}%\")\n",
    "    return model, history\n",
    "\n",
    "# Start the training\n",
    "CNN_MNIST, history = training(CNN_MNIST, train_loader, test_loader, loss_fcn, optimizer, epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3rFVqhg8Yd0"
   },
   "source": [
    "Now - we will quantize this model to INT8.\n",
    "\n",
    "NNCF enables post-training quantization (PTQ) by adding the quantization layers into the model graph and then using a subset of the training dataset to initialize the parameters of these additional quantization layers.\n",
    "\n",
    "By default PTQ uses an unannotated dataset to perform quantization. It uses representative dataset items to estimate the range of activation values in a network and then quantizes the network.\n",
    "\n",
    "Create an instance of `nncf.Dataset` class by passing two parameters:\n",
    "- data_source (PyTorch loader containing training samples)\n",
    "- transform_fn (to make data suitable for API).\n",
    "\n",
    "Call this instance `calibration_dataset`.\n",
    "\n",
    "Then, quantize `CNN_MNIST` model with `nncf.quantize()` function, which takes as input two parameters - the model and `calibration_dataset`. Call it `quantized_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWIfq-YZtbVY"
   },
   "outputs": [],
   "source": [
    "import nncf\n",
    "\n",
    "# Define the transformation function for the calibration dataset\n",
    "def transform_fn(data_item):\n",
    "    images, _ = data_item\n",
    "    return images\n",
    "\n",
    "# Create the calibration dataset\n",
    "calibration_dataset = nncf.Dataset(data_source=train_loader, transform_fn=transform_fn)\n",
    "\n",
    "# Quantize the CNN_MNIST model\n",
    "quantized_model = nncf.quantize(model=CNN_MNIST, calibration_dataset=calibration_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-khBK9_BfQ1"
   },
   "source": [
    "Finally, we will convert modes to OpenVINO Intermediate Representation (IR) format.\n",
    "\n",
    "OpenVINO IR is the proprietary model format of OpenVINO. It is produced after converting a model with model conversion API. Model conversion API translates the frequently used deep learning operations to their respective similar representation in OpenVINO and tunes them with the associated weights and biases from the trained model. The resulting IR contains two files:\n",
    "- `xml` - Describes the model topology.\n",
    "- `bin` - Contains the weights and binary data.\n",
    "\n",
    "To do that, we'll need `dummy_input` filled with random values and of size:\n",
    "\n",
    "`[batch_size, channel_number, image_shape[0], image_shape[1]]`\n",
    "\n",
    "Create `MNIST_fp32_ir` model with `ov.convert_model` that takes three parameters: the model, the dummy input and input size. Use `CNN_MNIST` model.\n",
    "\n",
    "Then, create `MNIST_int8_ir` model in the same way using `quantized_model`.\n",
    "\n",
    "Save both models to files (named `MNIST_fp32_ir.xml` and `MNIST_int8_ir.xml` respectively. Use `ov.save_model()` function.\n",
    "\n",
    "Finally - compile both models with `core.compile_model` function and use  `validate` function to calculate both models accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFyeUI2DBehj"
   },
   "outputs": [],
   "source": [
    "core = ov.Core()\n",
    "devices = core.available_devices\n",
    "\n",
    "dummy_input = torch.randn( ... )\n",
    "MNIST_fp32_ir = ov.convert_model(..., example_input=..., input=[-1, ..., ..., ...]) #TODO - FILL THE GAPS\n",
    "MNIST_int8_ir = ov.convert_model(..., example_input=..., input=[-1, ..., ..., ...]) #TODO - FILL THE GAPS\n",
    "ov.save_model(MNIST_fp32_ir, ...) #TODO - FILL THE FILENAME\n",
    "ov.save_model(MNIST_int8_ir, ...) #TODO - FILL THE FILENAME\n",
    "\n",
    "fp32_compiled_model = core.compile_model(MNIST_fp32_ir, devices[0])\n",
    "int8_compiled_model = core.compile_model(MNIST_int8_ir, devices[0])\n",
    "\n",
    "def validate(val_loader: torch.utils.data.DataLoader, model: Union[torch.nn.Module, CompiledModel], metric: BaseMetic):\n",
    "\n",
    "    # Switch to evaluate mode.\n",
    "    if not isinstance(model, CompiledModel):\n",
    "        model.eval()\n",
    "        model.to(torch_device)\n",
    "    total_accuracy = 0\n",
    "    samples_num = 0\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in tqdm.tqdm(enumerate(val_loader)):\n",
    "            images = images.to(torch_device)\n",
    "            target = target.to(torch_device)\n",
    "\n",
    "            # Compute the output.\n",
    "            if isinstance(model, CompiledModel):\n",
    "                output_layer = model.output(0)\n",
    "                output = model(images)[output_layer]\n",
    "                output = torch.from_numpy(output)\n",
    "            else:\n",
    "                output = model(images)\n",
    "\n",
    "            # Measure accuracy and record loss.\n",
    "            accuracy = metric(output, target)\n",
    "            total_accuracy += accuracy.item() * target.shape[0]\n",
    "            samples_num += target.shape[0]\n",
    "\n",
    "    return total_accuracy / samples_num\n",
    "\n",
    "acc1 = validate( ... )\n",
    "print(f'FP 32 model acc={acc1:.4f}')\n",
    "\n",
    "acc2 = validate( ... )\n",
    "print(f'INT 8 model acc={acc2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpb_tq9yEKBg"
   },
   "source": [
    "Is INT8 model accuracy similar to FP32 model accuracy? We should hope so!\n",
    "\n",
    "But let's verify what we have saved in terms of memory resources and network throughput!\n",
    "\n",
    "First, check the size of OpenVINO IR binary files. You saved both of them on your drive. Is the INT8 model smaller?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkT2VQWOy0BZ"
   },
   "outputs": [],
   "source": [
    "!ls -lh #BINARY_FILE_NAME\n",
    "!ls -lh #BINARY_FILE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vp6wdbtFD6C"
   },
   "source": [
    "Then, use the following code to benchmark both models. Is INT8 model faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQPqCagntsHK"
   },
   "outputs": [],
   "source": [
    "def parse_benchmark_output(benchmark_output: str):\n",
    "    \"\"\"Prints the output from benchmark_app in human-readable format\"\"\"\n",
    "    parsed_output = [line for line in benchmark_output if 'FPS' in line]\n",
    "    print(*parsed_output, sep='\\n')\n",
    "\n",
    "\n",
    "print('Benchmark FP32 model on CPU')\n",
    "benchmark_output = ! benchmark_app -m MNIST_fp32_ir.xml -d CPU -api async -t 15 -shape \"[1, 1, 28, 28]\"\n",
    "parse_benchmark_output(benchmark_output)\n",
    "\n",
    "print('Benchmark INT8 model on CPU')\n",
    "benchmark_output = ! benchmark_app -m MNIST_int8_ir.xml -d CPU -api async -t 15 -shape \"[1, 1, 28, 28]\"\n",
    "parse_benchmark_output(benchmark_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPe2aECWFmqF"
   },
   "source": [
    "Note, that we used very small network and we deal with very simple task. For bigger models and harder networks the perfomance and size differences can be even more significant!\n",
    "\n",
    "***Extention exercises***\n",
    "\n",
    "Read about `Quantizing with Accuracy Control` and try to use it for some pretrained network. Use `nncf.quantize_with_accuracy_control`. You can find pretrained networks with `torchvision.models`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpPQPePcGoDL"
   },
   "source": [
    "## Quantization-aware Training (QAT)\n",
    "\n",
    "Training-time model compression improves model performance by applying optimizations (such as quantization) during the training. The training process minimizes the loss associated with the lower-precision optimizations, so it is able to maintain the model’s accuracy while reducing its latency and memory footprint. Generally, training-time model optimization results in better model performance and accuracy than post-training optimization, but it can require more effort to set up.\n",
    "\n",
    "Quantization-aware Training is a popular method that allows quantizing a model and applying fine-tuning to restore accuracy degradation caused by quantization. In fact, this is the most accurate quantization method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4ns4TqBHTM_"
   },
   "source": [
    "For this part, let's use a bit harder Dataset. For MNIST, PTQ method was enough, right?\n",
    "\n",
    "Train your CNN model on CIFAR10 dataset for 10-20 epochs (google it!). Use the same training loops, metics, optimazers and loss function.\n",
    "\n",
    "Name the final trained model `CNN_CIFAR`, convert it to OpenVino IR and save to xml file.\n",
    "\n",
    "We start our QAT process with creating compressed models. Just use the following code (fill in the gaps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xdNDbJp09l3Q"
   },
   "outputs": [],
   "source": [
    "train_dataset = ...\n",
    "test_dataset = ...\n",
    "train_loader = ...\n",
    "test_loader = ...\n",
    "CNN_CIFAR, history = training( ... )\n",
    "\n",
    "# SAVE floating point model converted to OpenVino IR\n",
    "dummy_input = torch.randn(...) # Create dummy_input\n",
    "CIFAR_fp32_ir = ov.convert_model(...)\n",
    "ov.save_model(CIFAR_fp32_ir, ...) #TODO - FILL THE FILENAME\n",
    "\n",
    "# Compress model\n",
    "nncf_config_dict = {\n",
    "    \"input_info\": {\"sample_size\": [1, ..., ..., ...]}, #Put number of channels, image_size[0] and image_size[1]\n",
    "    \"compression\": {\n",
    "        \"algorithm\": \"quantization\",\n",
    "    },\n",
    "}\n",
    "nncf_config = NNCFConfig.from_dict(nncf_config_dict)\n",
    "nncf_config = register_default_init_args(nncf_config, train_loader)\n",
    "compression_ctrl, CNN_CIFAR_int8 = create_compressed_model(CNN_CIFAR, nncf_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wetn5RnaZzGx"
   },
   "source": [
    "We have our CIFAR CNN model ready to QAT. So... Just train it!\n",
    "\n",
    "Use your `training` function to train `CNN_CIFAR_int8` model for one more epoch!\n",
    "\n",
    "Thanks to OpenVINO API, after creating compressed model all we need to do is to continue training on INT8 model :) We call this process fine-tuning. It is applied to futher improve quantized model accuracy! Normally, several epochs of tuning are required with a small learning rate, the same that is usually used at the end of the training of the original model. No other changes in the training pipeline are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bj2Va0bwaW0i"
   },
   "outputs": [],
   "source": [
    "CNN_CIFAR_int8_finetuned, history = training( ... ) # just one epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKp3LbjUbBg-"
   },
   "source": [
    "Convert fine-tuned model to OpenVinoIR, save it to xml and verify both `CIFAR_fp32_ir` and `CIFAR_int8_ir` sizes.\n",
    "\n",
    "Is the INT8 network smaller?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "MigLfK-pLVqA"
   },
   "outputs": [],
   "source": [
    "core = ov.Core()\n",
    "devices = core.available_devices\n",
    "dummy_input = ...\n",
    "CIFAR_int8_ir = ov.convert_model( ... )\n",
    "ov.save_model( ... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aS9CujUULnz0"
   },
   "outputs": [],
   "source": [
    "!ls -lh ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbvVrwcCbZH_"
   },
   "source": [
    "Finally - compile models, validate and benchmark them.\n",
    "\n",
    "Did accuracy decreased?\n",
    "Is INT8 model faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1gbA8jbLidw"
   },
   "outputs": [],
   "source": [
    "fp32_cifar_compiled_model = core.compile_model( ... )\n",
    "int8_cifar_compiled_model = core.compile_model( ... )\n",
    "acc1 = validate( ... )\n",
    "print( ... )\n",
    "acc2 = validate( ... )\n",
    "print( ... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pmPBzNrbm7r"
   },
   "outputs": [],
   "source": [
    "def parse_benchmark_output(benchmark_output: str):\n",
    "    \"\"\"Prints the output from benchmark_app in human-readable format\"\"\"\n",
    "    parsed_output = [line for line in benchmark_output if 'FPS' in line]\n",
    "    print(*parsed_output, sep='\\n')\n",
    "\n",
    "\n",
    "print('Benchmark FP32 model on CPU')\n",
    "benchmark_output = ! benchmark_app -m CIFAR_fp32_ir.xml -d CPU -api async -t 15 -shape \"[1, 3, 32, 32]\"\n",
    "parse_benchmark_output(benchmark_output)\n",
    "\n",
    "print('Benchmark INT8 model on CPU')\n",
    "benchmark_output = ! benchmark_app -m CIFAR_int8_ir.xml -d CPU -api async -t 15 -shape \"[1, 3, 32, 32]\"\n",
    "parse_benchmark_output(benchmark_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wku_Ng6WcDdI"
   },
   "source": [
    "***Extention exercise***\n",
    "\n",
    "Compare PTQ and QAT. Create CNN model and:\n",
    "- train it for 20 epochs and save as `CNN_long.pth`\n",
    "- train it for 15 epochs and save as `CNN_short.pth`\n",
    "\n",
    "Then, apply PTQ on `CNN_long.pth` model and QAT (for 5 epochs) on `CNN_short.pth`. Compare the resulting models (in terms of accuracy, size and FPS)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
