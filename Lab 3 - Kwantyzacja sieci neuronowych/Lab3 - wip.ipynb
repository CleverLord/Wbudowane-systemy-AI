{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHGC0EFt3aZF"
   },
   "source": [
    "# Neural networks quantization\n",
    "\n",
    "Today we will deal with neural networks quantization!\n",
    "\n",
    "Remember that our goal is to reduce network size while keeping the accuracy high!\n",
    "\n",
    "For this purpose we will use Intel's OpenVino and Neural Network Compression Framework (NNCF). Be aware, that there are other frameworks to choose from: buildin PyTorch quantization, Brevitas from Xilinx, TensorRT and others.\n",
    "\n",
    "Use this link for OpenVino reference and documentation: https://docs.openvino.ai/2023.0/home.html\n",
    "\n",
    "First, install and import nessessery libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bNKDyp4Ssmcw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openvino in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (2023.1.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from openvino) (1.24.4)\n",
      "Requirement already satisfied: openvino-telemetry>=2023.1.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from openvino) (2023.2.1)\n",
      "Requirement already satisfied: nncf in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: jsonschema>=3.2.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from nncf) (4.19.1)\n",
      "Requirement already satisfied: jstyleson>=0.0.2 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from nncf) (0.0.2)\n",
      "Requirement already satisfied: natsort>=7.1.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from nncf) (8.4.0)\n",
      "Requirement already satisfied: networkx<=2.8.2,>=2.6 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from nncf) (2.8.2)\n",
      "Requirement already satisfied: ninja<1.11,>=1.10.0.post2 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from nncf) (1.10.2.4)\n",
      "Requirement already satisfied: numpy<1.25,>=1.19.1 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from nncf) (1.24.4)\n",
      "Requirement already satisfied: openvino-telemetry>=2023.1.1 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from nncf) (2023.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from nncf) (23.2)\n",
      "Requirement already satisfied: pandas<2.1,>=1.1.5 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from nncf) (2.0.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from nncf) (5.9.6)\n",
      "Requirement already satisfied: pydot>=1.4.1 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from nncf) (1.4.2)\n",
      "Requirement already satisfied: pymoo>=0.6.0.1 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from nncf) (0.6.0.1)\n",
      "Requirement already satisfied: pyparsing<3.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from nncf) (2.4.7)\n",
      "Requirement already satisfied: scikit-learn>=0.24.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from nncf) (1.3.2)\n",
      "Requirement already satisfied: scipy<1.11,>=1.3.2 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from nncf) (1.10.1)\n",
      "Requirement already satisfied: texttable>=1.6.3 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from nncf) (1.7.0)\n",
      "Requirement already satisfied: tqdm>=4.54.1 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from nncf) (4.66.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from jsonschema>=3.2.0->nncf) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from jsonschema>=3.2.0->nncf) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from jsonschema>=3.2.0->nncf) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from jsonschema>=3.2.0->nncf) (0.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from pandas<2.1,>=1.1.5->nncf) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from pandas<2.1,>=1.1.5->nncf) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from pandas<2.1,>=1.1.5->nncf) (2023.3)\n",
      "Requirement already satisfied: matplotlib>=3 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from pymoo>=0.6.0.1->nncf) (3.8.0)\n",
      "Requirement already satisfied: autograd>=1.4 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from pymoo>=0.6.0.1->nncf) (1.6.2)\n",
      "Requirement already satisfied: cma==3.2.2 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from pymoo>=0.6.0.1->nncf) (3.2.2)\n",
      "Requirement already satisfied: alive-progress in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from pymoo>=0.6.0.1->nncf) (3.1.4)\n",
      "Requirement already satisfied: dill in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from pymoo>=0.6.0.1->nncf) (0.3.7)\n",
      "Requirement already satisfied: Deprecated in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from pymoo>=0.6.0.1->nncf) (1.2.14)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from scikit-learn>=0.24.0->nncf) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from scikit-learn>=0.24.0->nncf) (3.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tqdm>=4.54.1->nncf) (0.4.6)\n",
      "Requirement already satisfied: future>=0.15.2 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from autograd>=1.4->pymoo>=0.6.0.1->nncf) (0.18.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf) (10.1.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf) (6.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<2.1,>=1.1.5->nncf) (1.16.0)\n",
      "Requirement already satisfied: about-time==4.2.1 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from alive-progress->pymoo>=0.6.0.1->nncf) (4.2.1)\n",
      "Requirement already satisfied: grapheme==0.6.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from alive-progress->pymoo>=0.6.0.1->nncf) (0.6.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from Deprecated->pymoo>=0.6.0.1->nncf) (1.15.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=3->pymoo>=0.6.0.1->nncf) (3.17.0)\n",
      "Requirement already satisfied: torch in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from torch) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (0.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from torchvision) (1.24.4)\n",
      "Requirement already satisfied: requests in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from torchvision) (2.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from torch==2.1.0->torchvision) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from torch==2.1.0->torchvision) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from torch==2.1.0->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from torch==2.1.0->torchvision) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from torch==2.1.0->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from torch==2.1.0->torchvision) (2023.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from requests->torchvision) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from requests->torchvision) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from jinja2->torch==2.1.0->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from sympy->torch==2.1.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from matplotlib) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from matplotlib) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from matplotlib) (6.1.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install openvino\n",
    "!pip3 install nncf\n",
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install matplotlib\n",
    "!pip3 install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1n_xcqlrsaMY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, openvino\n",
      "WARNING:nncf:NNCF provides best results with torch==2.0.1, while current torch version is 2.1.0+cpu. If you encounter issues, consider switching to torch==2.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nncf\n",
    "import openvino as ov\n",
    "import time\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "from nncf import NNCFConfig\n",
    "from nncf.torch import create_compressed_model, register_default_init_args\n",
    "from openvino.runtime.ie_api import CompiledModel\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, RandomRotation\n",
    "from typing import Union, List, Tuple, Any\n",
    "from abc import ABC, abstractmethod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEGYhjeZ453d"
   },
   "source": [
    "Let's start with...\n",
    "\n",
    "##Quantizing Models Post-training\n",
    "\n",
    "Post-training model optimization is the process of applying special methods that transform the model into a more hardware-friendly representation without retraining or fine-tuning. The most popular and widely-spread method here is 8-bit post-training quantization because it is:\n",
    "\n",
    "- It is easy-to-use.\n",
    "- It does not hurt accuracy a lot.\n",
    "- It provides significant performance improvement.\n",
    "- It suites many hardware available in stock since most of them support 8-bit computation natively.\n",
    "\n",
    "8-bit integer quantization lowers the precision of weights and activations to 8 bits, which leads to significant reduction in the model footprint and significant improvements in inference speed.\n",
    "\n",
    "Source: https://docs.openvino.ai/2023.0/ptq_introduction.html\n",
    "\n",
    "So first! We need a model to quantize.\n",
    "Reuse the CNN model from Laboratory 1 (along with training loops, metics, optimazers and loss function).\n",
    "\n",
    "Train it for 5 epochs with MNIST dataset. You should get around ~90% accuracy.\n",
    "\n",
    "Name the final trained model `CNN_MNIST`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7cnaAnDMsovx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_channels=1):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
    "        self.fc1 = nn.Linear(64*7*7 if input_channels==1 else 64*8*8, 128)  # MNIST is 28x28, CIFAR10 is 32x32\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Choose Dataset: 'MNIST' or 'CIFAR10'\n",
    "dataset_choice = 'MNIST'  # You can change this to 'MNIST' if needed\n",
    "\n",
    "# Set device\n",
    "torch_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Prepare datasets and dataloaders\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "if dataset_choice == 'MNIST':\n",
    "    train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "    test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform)\n",
    "    input_channels = 1\n",
    "else:\n",
    "    transform.transforms.append(transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)))\n",
    "    train_dataset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
    "    test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform)\n",
    "    input_channels = 3\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Model, metric, loss, and optimizer\n",
    "CNN_Model = CNN(input_channels=input_channels).to(torch_device)\n",
    "metric = nn.CrossEntropyLoss()\n",
    "loss_fcn = nn.CrossEntropyLoss().to(torch_device)\n",
    "optimizer = Adam(CNN_Model.parameters(), lr=0.001)\n",
    "\n",
    "# Train and test loops\n",
    "def train_test_pass(model, data_loader, loss_function, opt=None):\n",
    "    is_train = model.training\n",
    "    total_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    \n",
    "    for inputs, targets in data_loader:\n",
    "        inputs, targets = inputs.to(torch_device), targets.to(torch_device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        if is_train:\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_preds += (preds == targets).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    accuracy = 100.0 * correct_preds / len(data_loader.dataset)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def training(model, train_dl, test_dl, loss_fcn, optimizer, epochs=5):\n",
    "    history = {\"train_loss\": [], \"train_accuracy\": [], \"test_loss\": [], \"test_accuracy\": []}\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_accuracy = train_test_pass(model, train_dl, loss_fcn, optimizer)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_accuracy\"].append(train_accuracy)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss, test_accuracy = train_test_pass(model, test_dl, loss_fcn)\n",
    "            history[\"test_loss\"].append(test_loss)\n",
    "            history[\"test_accuracy\"].append(test_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} => \"\n",
    "              f\"Train loss: {train_loss:.4f}, Train accuracy: {train_accuracy:.2f}% | \"\n",
    "              f\"Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.2f}%\")\n",
    "    return model, history\n",
    "\n",
    "# File path to save and load the model\n",
    "MODEL_PATH = \"CNN_MNIST_model.pth\"\n",
    "\n",
    "# Check if the model file exists\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    # Load the model from the saved file\n",
    "    CNN_Model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    print(\"Model loaded from file.\")\n",
    "else:\n",
    "    pass\n",
    "    # Train the model\n",
    "    CNN_Model, history = training(CNN_Model, train_loader, test_loader, loss_fcn, optimizer, epochs=5)\n",
    "    # Save the trained model\n",
    "    torch.save(CNN_Model.state_dict(), MODEL_PATH)\n",
    "    print(f\"Model saved to {MODEL_PATH}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3rFVqhg8Yd0"
   },
   "source": [
    "Now - we will quantize this model to INT8.\n",
    "\n",
    "NNCF enables post-training quantization (PTQ) by adding the quantization layers into the model graph and then using a subset of the training dataset to initialize the parameters of these additional quantization layers.\n",
    "\n",
    "By default PTQ uses an unannotated dataset to perform quantization. It uses representative dataset items to estimate the range of activation values in a network and then quantizes the network.\n",
    "\n",
    "Create an instance of `nncf.Dataset` class by passing two parameters:\n",
    "- data_source (PyTorch loader containing training samples)\n",
    "- transform_fn (to make data suitable for API).\n",
    "\n",
    "Call this instance `calibration_dataset`.\n",
    "\n",
    "Then, quantize `CNN_MNIST` model with `nncf.quantize()` function, which takes as input two parameters - the model and `calibration_dataset`. Call it `quantized_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uWIfq-YZtbVY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Collecting tensor statistics |█               | 1 / 10\n",
      "INFO:nncf:Collecting tensor statistics |███             | 2 / 10\n",
      "INFO:nncf:Collecting tensor statistics |████            | 3 / 10\n",
      "INFO:nncf:Collecting tensor statistics |██████          | 4 / 10\n",
      "INFO:nncf:Collecting tensor statistics |████████        | 5 / 10\n",
      "INFO:nncf:Collecting tensor statistics |█████████       | 6 / 10\n",
      "INFO:nncf:Collecting tensor statistics |███████████     | 7 / 10\n",
      "INFO:nncf:Collecting tensor statistics |████████████    | 8 / 10\n",
      "INFO:nncf:Collecting tensor statistics |██████████████  | 9 / 10\n",
      "INFO:nncf:Collecting tensor statistics |████████████████| 10 / 10\n",
      "INFO:nncf:Compiling and loading torch extension: quantized_functions_cpu...\n",
      "WARNING:nncf:Could not compile CPU quantization extensions. Falling back on torch native operations - CPU quantization fine-tuning may be slower than expected.\n",
      "Reason: Command '['where', 'cl']' returned non-zero exit status 1.\n",
      "INFO:nncf:Finished loading torch extension: quantized_functions_cpu\n",
      "INFO:nncf:BatchNorm statistics adaptation |█               | 1 / 10\n",
      "INFO:nncf:BatchNorm statistics adaptation |███             | 2 / 10\n",
      "INFO:nncf:BatchNorm statistics adaptation |████            | 3 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krzys\\Documents\\GitHub\\Wbudowane-systemy-AI\\.venv\\lib\\site-packages\\torch\\utils\\cpp_extension.py:383: UserWarning: Error checking compiler version for cl: [WinError 2] Nie można odnaleźć określonego pliku\n",
      "  warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:BatchNorm statistics adaptation |██████          | 4 / 10\n",
      "INFO:nncf:BatchNorm statistics adaptation |████████        | 5 / 10\n",
      "INFO:nncf:BatchNorm statistics adaptation |█████████       | 6 / 10\n",
      "INFO:nncf:BatchNorm statistics adaptation |███████████     | 7 / 10\n",
      "INFO:nncf:BatchNorm statistics adaptation |████████████    | 8 / 10\n",
      "INFO:nncf:BatchNorm statistics adaptation |██████████████  | 9 / 10\n",
      "INFO:nncf:BatchNorm statistics adaptation |████████████████| 10 / 10\n"
     ]
    }
   ],
   "source": [
    "# Define the transformation function for the calibration dataset\n",
    "def transform_fn(data_item):\n",
    "    images, _ = data_item\n",
    "    return images\n",
    "\n",
    "# Create the calibration dataset\n",
    "calibration_dataset = nncf.Dataset(train_loader, transform_fn)\n",
    "\n",
    "# Quantize the CNN_MNIST model\n",
    "quantized_model = nncf.quantize(model=CNN_Model, calibration_dataset=calibration_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-khBK9_BfQ1"
   },
   "source": [
    "Finally, we will convert modes to OpenVINO Intermediate Representation (IR) format.\n",
    "\n",
    "OpenVINO IR is the proprietary model format of OpenVINO. It is produced after converting a model with model conversion API. Model conversion API translates the frequently used deep learning operations to their respective similar representation in OpenVINO and tunes them with the associated weights and biases from the trained model. The resulting IR contains two files:\n",
    "- `xml` - Describes the model topology.\n",
    "- `bin` - Contains the weights and binary data.\n",
    "\n",
    "To do that, we'll need `dummy_input` filled with random values and of size:\n",
    "\n",
    "`[batch_size, channel_number, image_shape[0], image_shape[1]]`\n",
    "\n",
    "Create `MNIST_fp32_ir` model with `ov.convert_model` that takes three parameters: the model, the dummy input and input size. Use `CNN_MNIST` model.\n",
    "\n",
    "Then, create `MNIST_int8_ir` model in the same way using `quantized_model`.\n",
    "\n",
    "Save both models to files (named `MNIST_fp32_ir.xml` and `MNIST_int8_ir.xml` respectively. Use `ov.save_model()` function.\n",
    "\n",
    "Finally - compile both models with `core.compile_model` function and use  `validate` function to calculate both models accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aFyeUI2DBehj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krzys\\Documents\\GitHub\\Wbudowane-systemy-AI\\.venv\\lib\\site-packages\\nncf\\torch\\quantization\\layers.py:336: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return self._level_low.item()\n",
      "c:\\Users\\krzys\\Documents\\GitHub\\Wbudowane-systemy-AI\\.venv\\lib\\site-packages\\nncf\\torch\\quantization\\layers.py:344: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return self._level_high.item()\n",
      "313it [00:02, 142.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP 32 model acc=0.0297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:01, 204.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INT 8 model acc=0.0297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a dummy input for the model.\n",
    "# Assuming MNIST images are 1x28x28 and a batch size of 32\n",
    "dummy_input = torch.randn(32, 1, 28, 28)\n",
    "\n",
    "# 2. Convert models to OpenVINO IR format\n",
    "MNIST_fp32_ir = ov.convert_model(CNN_Model, example_input=dummy_input, input=[-1, 1, 28, 28])\n",
    "MNIST_int8_ir = ov.convert_model(quantized_model, example_input=dummy_input, input=[-1, 1, 28, 28])\n",
    "\n",
    "# 3. Save models to XML files\n",
    "ov.save_model(MNIST_fp32_ir, \"MNIST_fp32_ir.xml\")\n",
    "ov.save_model(MNIST_int8_ir, \"MNIST_int8_ir.xml\")\n",
    "\n",
    "# 4. Compile models\n",
    "core = ov.Core()\n",
    "devices = core.available_devices\n",
    "fp32_compiled_model = core.compile_model(MNIST_fp32_ir, devices[0])\n",
    "int8_compiled_model = core.compile_model(MNIST_int8_ir, devices[0])\n",
    "\n",
    "# BaseMetric placeholder (assuming you have this defined elsewhere or imported it)\n",
    "# If you haven't defined it yet, replace BaseMetric with the correct metric class/type.\n",
    "class BaseMetric:\n",
    "    # Placeholder class, replace with the actual metric class or import from the relevant module.\n",
    "    pass\n",
    "\n",
    "def validate(val_loader: torch.utils.data.DataLoader, model: Union[torch.nn.Module, CompiledModel], metric: BaseMetric):\n",
    "\n",
    "    # Switch to evaluate mode.\n",
    "    if not isinstance(model, CompiledModel):\n",
    "        model.eval()\n",
    "        model.to(torch_device)\n",
    "    total_accuracy = 0\n",
    "    samples_num = 0\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in tqdm.tqdm(enumerate(val_loader)):\n",
    "            images = images.to(torch_device)\n",
    "            target = target.to(torch_device)\n",
    "\n",
    "            # Compute the output.\n",
    "            if isinstance(model, CompiledModel):\n",
    "                output_layer = model.output(0)\n",
    "                output = model(images)[output_layer]\n",
    "                output = torch.from_numpy(output)\n",
    "            else:\n",
    "                output = model(images)\n",
    "\n",
    "            # Measure accuracy and record loss.\n",
    "            accuracy = metric(output, target)\n",
    "            total_accuracy += accuracy.item() * target.shape[0]\n",
    "            samples_num += target.shape[0]\n",
    "\n",
    "    return total_accuracy / samples_num\n",
    "\n",
    "\n",
    "# 5. Calculate and print accuracy for both models using the validate function\n",
    "acc1 = validate(test_loader, fp32_compiled_model, metric)\n",
    "print(f'FP 32 model acc={acc1:.4f}')\n",
    "\n",
    "acc2 = validate(test_loader, int8_compiled_model, metric)\n",
    "print(f'INT 8 model acc={acc2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpb_tq9yEKBg"
   },
   "source": [
    "Is INT8 model accuracy similar to FP32 model accuracy? We should hope so!\n",
    "\n",
    "But let's verify what we have saved in terms of memory resources and network throughput!\n",
    "\n",
    "First, check the size of OpenVINO IR binary files. You saved both of them on your drive. Is the INT8 model smaller?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UkT2VQWOy0BZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of FP32 model: 0.87 MB\n",
      "Size of INT8 model: 0.43 MB\n",
      "Compression ratio: 2.00\n"
     ]
    }
   ],
   "source": [
    "# !ls -lh MNIST_fp32_ir.bin\n",
    "# !ls -lh MNIST_int8_ir.bin\n",
    "import os\n",
    "\n",
    "# Get file sizes\n",
    "size_fp32 = os.path.getsize(\"MNIST_fp32_ir.bin\")\n",
    "size_int8 = os.path.getsize(\"MNIST_int8_ir.bin\")\n",
    "\n",
    "# Calculate the compression ratio\n",
    "compression_ratio = size_fp32 / size_int8\n",
    "\n",
    "print(f\"Size of FP32 model: {size_fp32 / (1024 * 1024):.2f} MB\")\n",
    "print(f\"Size of INT8 model: {size_int8 / (1024 * 1024):.2f} MB\")\n",
    "print(f\"Compression ratio: {compression_ratio:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vp6wdbtFD6C"
   },
   "source": [
    "Then, use the following code to benchmark both models. Is INT8 model faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jQPqCagntsHK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark FP32 model on CPU\n",
      "[ INFO ] Throughput:   14808.75 FPS\n",
      "Benchmark INT8 model on CPU\n",
      "[ INFO ] Throughput:   23991.51 FPS\n",
      "Benchmark FP32 model on GPU\n"
     ]
    }
   ],
   "source": [
    "# def parse_benchmark_output(benchmark_output: str):\n",
    "#     \"\"\"Prints the output from benchmark_app in human-readable format\"\"\"\n",
    "#     parsed_output = [line for line in benchmark_output if 'FPS' in line]\n",
    "#     print(*parsed_output, sep='\\n')\n",
    "\n",
    "\n",
    "# print('Benchmark FP32 model on CPU')\n",
    "# benchmark_output = ! benchmark_app -m MNIST_fp32_ir.xml -d CPU -api async -t 15 -shape \"[1, 1, 28, 28]\"\n",
    "# parse_benchmark_output(benchmark_output)\n",
    "\n",
    "# print('Benchmark INT8 model on CPU')\n",
    "# benchmark_output = ! benchmark_app -m MNIST_int8_ir.xml -d CPU -api async -t 15 -shape \"[1, 1, 28, 28]\"\n",
    "# parse_benchmark_output(benchmark_output)\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def parse_benchmark_output(benchmark_output: str):\n",
    "    \"\"\"Prints the output from benchmark_app in human-readable format\"\"\"\n",
    "    parsed_output = [line for line in benchmark_output.decode('utf-8').split('\\n') if 'FPS' in line]\n",
    "    print(*parsed_output, sep='\\n')\n",
    "\n",
    "def run_benchmark(model_file: str, device: str = \"CPU\"):\n",
    "    \"\"\"Runs benchmark_app for a given model on the specified device and returns its output\"\"\"\n",
    "    command = [\n",
    "        \"benchmark_app\",\n",
    "        \"-m\", model_file,\n",
    "        \"-d\", device,\n",
    "        \"-api\", \"async\",\n",
    "        \"-t\", \"5\",\n",
    "        \"-shape\", \"[1, 1, 28, 28]\"\n",
    "    ]\n",
    "    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    return result.stdout\n",
    "\n",
    "\n",
    "print('Benchmark FP32 model on CPU')\n",
    "benchmark_output = run_benchmark(\"MNIST_fp32_ir.xml\")\n",
    "parse_benchmark_output(benchmark_output)\n",
    "\n",
    "print('Benchmark INT8 model on CPU')\n",
    "benchmark_output = run_benchmark(\"MNIST_int8_ir.xml\")\n",
    "parse_benchmark_output(benchmark_output)\n",
    "\n",
    "print('Benchmark FP32 model on GPU')\n",
    "benchmark_output = run_benchmark(\"MNIST_fp32_ir.xml\", device=\"GPU\")\n",
    "parse_benchmark_output(benchmark_output)\n",
    "\n",
    "print('Benchmark INT8 model on GPU')\n",
    "benchmark_output = run_benchmark(\"MNIST_int8_ir.xml\", device=\"GPU\")\n",
    "parse_benchmark_output(benchmark_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPe2aECWFmqF"
   },
   "source": [
    "Note, that we used very small network and we deal with very simple task. For bigger models and harder networks the perfomance and size differences can be even more significant!\n",
    "\n",
    "***Extention exercises***\n",
    "\n",
    "Read about `Quantizing with Accuracy Control` and try to use it for some pretrained network. Use `nncf.quantize_with_accuracy_control`. You can find pretrained networks with `torchvision.models`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpPQPePcGoDL"
   },
   "source": [
    "## Quantization-aware Training (QAT)\n",
    "\n",
    "Training-time model compression improves model performance by applying optimizations (such as quantization) during the training. The training process minimizes the loss associated with the lower-precision optimizations, so it is able to maintain the model’s accuracy while reducing its latency and memory footprint. Generally, training-time model optimization results in better model performance and accuracy than post-training optimization, but it can require more effort to set up.\n",
    "\n",
    "Quantization-aware Training is a popular method that allows quantizing a model and applying fine-tuning to restore accuracy degradation caused by quantization. In fact, this is the most accurate quantization method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4ns4TqBHTM_"
   },
   "source": [
    "For this part, let's use a bit harder Dataset. For MNIST, PTQ method was enough, right?\n",
    "\n",
    "Train your CNN model on CIFAR10 dataset for 10-20 epochs (google it!). Use the same training loops, metics, optimazers and loss function.\n",
    "\n",
    "Name the final trained model `CNN_CIFAR`, convert it to OpenVino IR and save to xml file.\n",
    "\n",
    "We start our QAT process with creating compressed models. Just use the following code (fill in the gaps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xdNDbJp09l3Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/5 => Train loss: 2.3008, Train accuracy: 11.55% | Test loss: 2.3009, Test accuracy: 11.58%\n",
      "Epoch 2/5 => Train loss: 2.3008, Train accuracy: 11.55% | Test loss: 2.3009, Test accuracy: 11.58%\n",
      "Epoch 3/5 => Train loss: 2.3008, Train accuracy: 11.55% | Test loss: 2.3009, Test accuracy: 11.58%\n",
      "Epoch 4/5 => Train loss: 2.3008, Train accuracy: 11.55% | Test loss: 2.3009, Test accuracy: 11.58%\n",
      "Epoch 5/5 => Train loss: 2.3008, Train accuracy: 11.55% | Test loss: 2.3009, Test accuracy: 11.58%\n",
      "WARNING:nncf:Enabling quantization range initialization with default parameters.\n",
      "INFO:nncf:Collecting tensor statistics |████            | 1 / 4\n",
      "INFO:nncf:Collecting tensor statistics |████████        | 2 / 4\n",
      "INFO:nncf:Collecting tensor statistics |████████████    | 3 / 4\n",
      "INFO:nncf:Collecting tensor statistics |████████████████| 4 / 4\n",
      "INFO:nncf:BatchNorm statistics adaptation |█               | 3 / 32\n",
      "INFO:nncf:BatchNorm statistics adaptation |███             | 6 / 32\n",
      "INFO:nncf:BatchNorm statistics adaptation |████            | 9 / 32\n",
      "INFO:nncf:BatchNorm statistics adaptation |██████          | 12 / 32\n",
      "INFO:nncf:BatchNorm statistics adaptation |███████         | 15 / 32\n",
      "INFO:nncf:BatchNorm statistics adaptation |█████████       | 18 / 32\n",
      "INFO:nncf:BatchNorm statistics adaptation |██████████      | 21 / 32\n",
      "INFO:nncf:BatchNorm statistics adaptation |████████████    | 24 / 32\n",
      "INFO:nncf:BatchNorm statistics adaptation |█████████████   | 27 / 32\n",
      "INFO:nncf:BatchNorm statistics adaptation |███████████████ | 30 / 32\n",
      "INFO:nncf:BatchNorm statistics adaptation |████████████████| 32 / 32\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Set up CIFAR10 data loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(CNN(input_channels=3).parameters(), lr=0.01)\n",
    "\n",
    "# Train the CNN model using CIFAR10\n",
    "CNN_CIFAR, history = training(CNN(input_channels=3), train_loader, test_loader, loss_fcn, optimizer, epochs=5)\n",
    "\n",
    "# Convert trained model to OpenVINO IR and save\n",
    "dummy_input = torch.randn([1, 3, 32, 32])  # CIFAR10 images are 3x32x32\n",
    "CIFAR_fp32_ir = ov.convert_model(CNN_CIFAR, example_input=dummy_input, input=[-1, 3, 32, 32])\n",
    "ov.save_model(CIFAR_fp32_ir, \"CIFAR_fp32_ir.xml\")\n",
    "\n",
    "# Compress model using NNCF for Quantization-aware Training\n",
    "nncf_config_dict = {\n",
    "    \"input_info\": {\"sample_size\": [1, 3, 32, 32]},\n",
    "    \"compression\": {\n",
    "        \"algorithm\": \"quantization\",\n",
    "    },\n",
    "}\n",
    "nncf_config = NNCFConfig.from_dict(nncf_config_dict)\n",
    "nncf_config = register_default_init_args(nncf_config, train_loader)\n",
    "compression_ctrl, CNN_CIFAR_int8 = create_compressed_model(CNN_CIFAR, nncf_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wetn5RnaZzGx"
   },
   "source": [
    "We have our CIFAR CNN model ready to QAT. So... Just train it!\n",
    "\n",
    "Use your `training` function to train `CNN_CIFAR_int8` model for one more epoch!\n",
    "\n",
    "Thanks to OpenVINO API, after creating compressed model all we need to do is to continue training on INT8 model :) We call this process fine-tuning. It is applied to futher improve quantized model accuracy! Normally, several epochs of tuning are required with a small learning rate, the same that is usually used at the end of the training of the original model. No other changes in the training pipeline are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bj2Va0bwaW0i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 => Train loss: 2.1401, Train accuracy: 39.45% | Test loss: 2.1305, Test accuracy: 45.04%\n",
      "Epoch 2/5 => Train loss: 2.2454, Train accuracy: 26.77% | Test loss: 2.2967, Test accuracy: 18.92%\n",
      "Epoch 3/5 => Train loss: 2.2970, Train accuracy: 18.91% | Test loss: 2.2975, Test accuracy: 18.88%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\krzys\\Documents\\GitHub\\Wbudowane-systemy-AI\\Lab 3 - Kwantyzacja sieci neuronowych\\Lab3 - wip.ipynb Cell 19\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/krzys/Documents/GitHub/Wbudowane-systemy-AI/Lab%203%20-%20Kwantyzacja%20sieci%20neuronowych/Lab3%20-%20wip.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m optimizer_finetune \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(CNN_CIFAR_int8\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/krzys/Documents/GitHub/Wbudowane-systemy-AI/Lab%203%20-%20Kwantyzacja%20sieci%20neuronowych/Lab3%20-%20wip.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Fine-tuning the quantized model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/krzys/Documents/GitHub/Wbudowane-systemy-AI/Lab%203%20-%20Kwantyzacja%20sieci%20neuronowych/Lab3%20-%20wip.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m CNN_CIFAR_int8_finetuned, history \u001b[39m=\u001b[39m training(CNN_CIFAR_int8, train_loader, test_loader, loss_fcn_finetune, optimizer_finetune, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\krzys\\Documents\\GitHub\\Wbudowane-systemy-AI\\Lab 3 - Kwantyzacja sieci neuronowych\\Lab3 - wip.ipynb Cell 19\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/krzys/Documents/GitHub/Wbudowane-systemy-AI/Lab%203%20-%20Kwantyzacja%20sieci%20neuronowych/Lab3%20-%20wip.ipynb#X24sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/krzys/Documents/GitHub/Wbudowane-systemy-AI/Lab%203%20-%20Kwantyzacja%20sieci%20neuronowych/Lab3%20-%20wip.ipynb#X24sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/krzys/Documents/GitHub/Wbudowane-systemy-AI/Lab%203%20-%20Kwantyzacja%20sieci%20neuronowych/Lab3%20-%20wip.ipynb#X24sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     train_loss, train_accuracy \u001b[39m=\u001b[39m train_test_pass(model, train_dl, loss_fcn, optimizer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/krzys/Documents/GitHub/Wbudowane-systemy-AI/Lab%203%20-%20Kwantyzacja%20sieci%20neuronowych/Lab3%20-%20wip.ipynb#X24sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m     history[\u001b[39m\"\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/krzys/Documents/GitHub/Wbudowane-systemy-AI/Lab%203%20-%20Kwantyzacja%20sieci%20neuronowych/Lab3%20-%20wip.ipynb#X24sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m     history[\u001b[39m\"\u001b[39m\u001b[39mtrain_accuracy\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(train_accuracy)\n",
      "\u001b[1;32mc:\\Users\\krzys\\Documents\\GitHub\\Wbudowane-systemy-AI\\Lab 3 - Kwantyzacja sieci neuronowych\\Lab3 - wip.ipynb Cell 19\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/krzys/Documents/GitHub/Wbudowane-systemy-AI/Lab%203%20-%20Kwantyzacja%20sieci%20neuronowych/Lab3%20-%20wip.ipynb#X24sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mfor\u001b[39;00m inputs, targets \u001b[39min\u001b[39;00m data_loader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/krzys/Documents/GitHub/Wbudowane-systemy-AI/Lab%203%20-%20Kwantyzacja%20sieci%20neuronowych/Lab3%20-%20wip.ipynb#X24sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     inputs, targets \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(torch_device), targets\u001b[39m.\u001b[39mto(torch_device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/krzys/Documents/GitHub/Wbudowane-systemy-AI/Lab%203%20-%20Kwantyzacja%20sieci%20neuronowych/Lab3%20-%20wip.ipynb#X24sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/krzys/Documents/GitHub/Wbudowane-systemy-AI/Lab%203%20-%20Kwantyzacja%20sieci%20neuronowych/Lab3%20-%20wip.ipynb#X24sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_function(outputs, targets)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/krzys/Documents/GitHub/Wbudowane-systemy-AI/Lab%203%20-%20Kwantyzacja%20sieci%20neuronowych/Lab3%20-%20wip.ipynb#X24sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m inputs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\krzys\\Documents\\GitHub\\Wbudowane-systemy-AI\\.venv\\lib\\site-packages\\nncf\\torch\\nncf_network.py:918\u001b[0m, in \u001b[0;36mNNCFNetwork.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    914\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[39m    Ensures that functor-like calls of the processed model object will directly trigger the NNCF-specific\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[39m    forward call.\u001b[39;00m\n\u001b[0;32m    917\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     \u001b[39mreturn\u001b[39;00m ORIGINAL_CALL(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\krzys\\Documents\\GitHub\\Wbudowane-systemy-AI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\krzys\\Documents\\GitHub\\Wbudowane-systemy-AI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krzys\\Documents\\GitHub\\Wbudowane-systemy-AI\\.venv\\lib\\site-packages\\nncf\\torch\\nncf_network.py:945\u001b[0m, in \u001b[0;36mNNCFNetwork.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    942\u001b[0m     retval \u001b[39m=\u001b[39m wrap_module_call(_unbound_like_original_instance_forward)(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    944\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnncf\u001b[39m.\u001b[39m_bound_original_forward \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 945\u001b[0m     retval \u001b[39m=\u001b[39m wrap_module_call(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnncf\u001b[39m.\u001b[39m_original_unbound_forward)(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    946\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m_unbound_like_original_forward\u001b[39m(_self, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "File \u001b[1;32mc:\\Users\\krzys\\Documents\\GitHub\\Wbudowane-systemy-AI\\.venv\\lib\\site-packages\\nncf\\torch\\dynamic_graph\\wrappers.py:151\u001b[0m, in \u001b[0;36mwrap_module_call.<locals>.wrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m         ctx\u001b[39m.\u001b[39min_skipped_block \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m     retval \u001b[39m=\u001b[39m module_call(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m ITERATION_MODULES\u001b[39m.\u001b[39mregistry_dict:\n\u001b[0;32m    154\u001b[0m     ctx\u001b[39m.\u001b[39mreset_operator_call_count_in_scope(ctx\u001b[39m.\u001b[39mscope)\n",
      "\u001b[1;32mc:\\Users\\krzys\\Documents\\GitHub\\Wbudowane-systemy-AI\\Lab 3 - Kwantyzacja sieci neuronowych\\Lab3 - wip.ipynb Cell 19\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/krzys/Documents/GitHub/Wbudowane-systemy-AI/Lab%203%20-%20Kwantyzacja%20sieci%20neuronowych/Lab3%20-%20wip.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/krzys/Documents/GitHub/Wbudowane-systemy-AI/Lab%203%20-%20Kwantyzacja%20sieci%20neuronowych/Lab3%20-%20wip.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmax_pool2d(x, \u001b[39m2\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/krzys/Documents/GitHub/Wbudowane-systemy-AI/Lab%203%20-%20Kwantyzacja%20sieci%20neuronowych/Lab3%20-%20wip.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/krzys/Documents/GitHub/Wbudowane-systemy-AI/Lab%203%20-%20Kwantyzacja%20sieci%20neuronowych/Lab3%20-%20wip.ipynb#X24sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmax_pool2d(x, \u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/krzys/Documents/GitHub/Wbudowane-systemy-AI/Lab%203%20-%20Kwantyzacja%20sieci%20neuronowych/Lab3%20-%20wip.ipynb#X24sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\krzys\\Documents\\GitHub\\Wbudowane-systemy-AI\\.venv\\lib\\site-packages\\nncf\\torch\\dynamic_graph\\wrappers.py:151\u001b[0m, in \u001b[0;36mwrap_module_call.<locals>.wrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m         ctx\u001b[39m.\u001b[39min_skipped_block \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m     retval \u001b[39m=\u001b[39m module_call(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m ITERATION_MODULES\u001b[39m.\u001b[39mregistry_dict:\n\u001b[0;32m    154\u001b[0m     ctx\u001b[39m.\u001b[39mreset_operator_call_count_in_scope(ctx\u001b[39m.\u001b[39mscope)\n",
      "File \u001b[1;32mc:\\Users\\krzys\\Documents\\GitHub\\Wbudowane-systemy-AI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\krzys\\Documents\\GitHub\\Wbudowane-systemy-AI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krzys\\Documents\\GitHub\\Wbudowane-systemy-AI\\.venv\\lib\\site-packages\\nncf\\torch\\layer_utils.py:93\u001b[0m, in \u001b[0;36m_NNCFModuleMixin.forward\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m     91\u001b[0m         args \u001b[39m=\u001b[39m op_args\n\u001b[0;32m     92\u001b[0m forward_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_custom_forward_fn\u001b[39m.\u001b[39m\u001b[39m__func__\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_custom_forward_fn \u001b[39melse\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mforward\u001b[39m.\u001b[39m\u001b[39m__func__\u001b[39m\n\u001b[1;32m---> 93\u001b[0m results \u001b[39m=\u001b[39m forward_fn(proxy_module, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     94\u001b[0m \u001b[39mfor\u001b[39;00m op \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_ops\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m     95\u001b[0m     op_results \u001b[39m=\u001b[39m op(proxy_module, results)\n",
      "File \u001b[1;32mc:\\Users\\krzys\\Documents\\GitHub\\Wbudowane-systemy-AI\\.venv\\lib\\site-packages\\nncf\\torch\\layers.py:158\u001b[0m, in \u001b[0;36mNNCFConv2d._custom_forward_fn\u001b[1;34m(self, input_)\u001b[0m\n\u001b[0;32m    156\u001b[0m proxy_padding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding\n\u001b[0;32m    157\u001b[0m proxy_num_groups \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups\n\u001b[1;32m--> 158\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward_proxy(\n\u001b[0;32m    159\u001b[0m     input_, proxy_weight, proxy_bias, proxy_padding_value, proxy_padding, proxy_num_groups\n\u001b[0;32m    160\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\krzys\\Documents\\GitHub\\Wbudowane-systemy-AI\\.venv\\lib\\site-packages\\nncf\\torch\\layers.py:188\u001b[0m, in \u001b[0;36mNNCFConv2d._conv_forward_proxy\u001b[1;34m(self, input_, weight, bias, padding_value, padding, num_groups)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\n\u001b[0;32m    179\u001b[0m         F\u001b[39m.\u001b[39mpad(input_, reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode, value\u001b[39m=\u001b[39mpadding_val),\n\u001b[0;32m    180\u001b[0m         weight,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups,\n\u001b[0;32m    186\u001b[0m     )\n\u001b[0;32m    187\u001b[0m \u001b[39mif\u001b[39;00m padding_val \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 188\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(input_, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, padding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n\u001b[0;32m    189\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\n\u001b[0;32m    190\u001b[0m     F\u001b[39m.\u001b[39mpad(input_, reversed_padding_repeated_twice, value\u001b[39m=\u001b[39mpadding_val),\n\u001b[0;32m    191\u001b[0m     weight,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups,\n\u001b[0;32m    197\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\krzys\\Documents\\GitHub\\Wbudowane-systemy-AI\\.venv\\lib\\site-packages\\nncf\\torch\\dynamic_graph\\wrappers.py:94\u001b[0m, in \u001b[0;36mwrap_operator.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m     result \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39mtensor_cache\n\u001b[0;32m     93\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 94\u001b[0m     result \u001b[39m=\u001b[39m _execute_op(op_address, operator_info, operator, ctx, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     96\u001b[0m str_op_address \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(op_address)\n\u001b[0;32m     97\u001b[0m \u001b[39mif\u001b[39;00m str_op_address \u001b[39min\u001b[39;00m ctx\u001b[39m.\u001b[39mend_node_name_of_skipped_block:\n",
      "File \u001b[1;32mc:\\Users\\krzys\\Documents\\GitHub\\Wbudowane-systemy-AI\\.venv\\lib\\site-packages\\nncf\\torch\\dynamic_graph\\wrappers.py:175\u001b[0m, in \u001b[0;36m_execute_op\u001b[1;34m(op_address, operator_info, operator, ctx, *args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m args \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(processed_input\u001b[39m.\u001b[39mop_args)\n\u001b[0;32m    174\u001b[0m kwargs \u001b[39m=\u001b[39m processed_input\u001b[39m.\u001b[39mop_kwargs\n\u001b[1;32m--> 175\u001b[0m result \u001b[39m=\u001b[39m operator(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    176\u001b[0m node \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mtype\u001b[39m(\u001b[39mNotImplemented\u001b[39m)):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the loss function\n",
    "loss_fcn_finetune = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer with a smaller learning rate for fine-tuning\n",
    "optimizer_finetune = torch.optim.Adam(CNN_CIFAR_int8.parameters(), lr=0.0001)\n",
    "\n",
    "# Fine-tuning the quantized model\n",
    "CNN_CIFAR_int8_finetuned, history = training(CNN_CIFAR_int8, train_loader, test_loader, loss_fcn_finetune, optimizer_finetune, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKp3LbjUbBg-"
   },
   "source": [
    "Convert fine-tuned model to OpenVinoIR, save it to xml and verify both `CIFAR_fp32_ir` and `CIFAR_int8_ir` sizes.\n",
    "\n",
    "Is the INT8 network smaller?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MigLfK-pLVqA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Convert the fine-tuned model to OpenVINO IR\n",
    "dummy_input = torch.randn([1, 3, 32, 32])  # CIFAR10 images are 3x32x32\n",
    "CIFAR_int8_ir = ov.convert_model(CNN_CIFAR_int8_finetuned, example_input=dummy_input, input=[-1, 3, 32, 32])\n",
    "\n",
    "# Save the INT8 model to XML\n",
    "ov.save_model(CIFAR_int8_ir, \"CIFAR_int8_ir.xml\")\n",
    "\n",
    "# Verify the sizes of the FP32 and INT8 IR models\n",
    "fp32_size = os.path.getsize(\"CIFAR_fp32_ir.xml\")\n",
    "int8_size = os.path.getsize(\"CIFAR_int8_ir.xml\")\n",
    "\n",
    "print(f\"Size of CIFAR_fp32_ir.xml: {fp32_size / (1024*1024):.2f} MB\")\n",
    "print(f\"Size of CIFAR_int8_ir.xml: {int8_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Compare the sizes\n",
    "if int8_size < fp32_size:\n",
    "    print(\"The INT8 network is smaller!\")\n",
    "else:\n",
    "    print(\"The INT8 network is not smaller.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbvVrwcCbZH_"
   },
   "source": [
    "Finally - compile models, validate and benchmark them.\n",
    "\n",
    "Did accuracy decreased?\n",
    "Is INT8 model faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pmPBzNrbm7r"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def validate(compiled_model, data_loader, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Switch to evaluation mode\n",
    "    compiled_model.switch_to_eval_mode()\n",
    "    \n",
    "    for inputs, targets in data_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Use OpenVINO's inference engine\n",
    "        outputs = compiled_model.infer(inputs)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Ensure you have loaded the models in OpenVINO IR format\n",
    "fp32_cifar_ir_model = ... # Load the fp32 model\n",
    "int8_cifar_ir_model = ... # Load the int8 model\n",
    "\n",
    "device = \"CPU\"  # Choose the device, can be \"GPU\", \"MYRIAD\", etc.\n",
    "\n",
    "# Compile the models\n",
    "fp32_cifar_compiled_model = core.compile_model(fp32_cifar_ir_model, device)\n",
    "int8_cifar_compiled_model = core.compile_model(int8_cifar_ir_model, device)\n",
    "\n",
    "# Validate and benchmark\n",
    "start_time = time.time()\n",
    "acc1 = validate(fp32_cifar_compiled_model, test_loader, device)\n",
    "fp32_time = time.time() - start_time\n",
    "\n",
    "print(f\"FP32 Model Accuracy: {acc1:.2f}%\")\n",
    "print(f\"FP32 Model Inference Time: {fp32_time:.4f} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "acc2 = validate(int8_cifar_compiled_model, test_loader, device)\n",
    "int8_time = time.time() - start_time\n",
    "\n",
    "print(f\"INT8 Model Accuracy: {acc2:.2f}%\")\n",
    "print(f\"INT8 Model Inference Time: {int8_time:.4f} seconds\")\n",
    "\n",
    "# Check if accuracy decreased for INT8\n",
    "if acc2 < acc1:\n",
    "    print(\"Accuracy decreased with INT8 quantization.\")\n",
    "else:\n",
    "    print(\"Accuracy remained stable or increased with INT8 quantization.\")\n",
    "\n",
    "# Check if INT8 model is faster\n",
    "if int8_time < fp32_time:\n",
    "    print(\"INT8 model is faster!\")\n",
    "else:\n",
    "    print(\"FP32 model is faster or there's no significant difference.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wku_Ng6WcDdI"
   },
   "source": [
    "***Extention exercise***\n",
    "\n",
    "Compare PTQ and QAT. Create CNN model and:\n",
    "- train it for 20 epochs and save as `CNN_long.pth`\n",
    "- train it for 15 epochs and save as `CNN_short.pth`\n",
    "\n",
    "Then, apply PTQ on `CNN_long.pth` model and QAT (for 5 epochs) on `CNN_short.pth`. Compare the resulting models (in terms of accuracy, size and FPS)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
