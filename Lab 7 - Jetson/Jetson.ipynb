{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHGC0EFt3aZF"
      },
      "source": [
        "# CNN inference with eGPU - part 2\n",
        "\n",
        "Today we'll run neural networks inference on NVIDIA Jetson Nano eGPUs.\n",
        "\n",
        "You can find out more here: https://www.nvidia.com/pl-pl/autonomous-machines/embedded-systems/jetson-nano-developer-kit/\n",
        "\n",
        "First, install and import nessessery libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3fkUb7mzgmGD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-datasets in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (4.8.3)\n",
            "Requirement already satisfied: absl-py in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-datasets) (1.4.0)\n",
            "Requirement already satisfied: click in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-datasets) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-datasets) (0.1.8)\n",
            "Requirement already satisfied: etils>=0.9.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (1.5.2)\n",
            "Requirement already satisfied: numpy in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-datasets) (1.24.4)\n",
            "Requirement already satisfied: promise in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-datasets) (3.19.6)\n",
            "Requirement already satisfied: psutil in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-datasets) (5.9.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-datasets) (2.31.0)\n",
            "Requirement already satisfied: tensorflow-metadata in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-datasets) (1.13.0)\n",
            "Requirement already satisfied: termcolor in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-datasets) (2.3.0)\n",
            "Requirement already satisfied: toml in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-datasets) (4.66.1)\n",
            "Requirement already satisfied: wrapt in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-datasets) (1.15.0)\n",
            "Requirement already satisfied: fsspec in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (2023.10.0)\n",
            "Requirement already satisfied: importlib_resources in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (6.1.0)\n",
            "Requirement already satisfied: typing_extensions in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (4.8.0)\n",
            "Requirement already satisfied: zipp in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2023.7.22)\n",
            "Requirement already satisfied: colorama in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from click->tensorflow-datasets) (0.4.6)\n",
            "Requirement already satisfied: six in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from promise->tensorflow-datasets) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\users\\krzys\\documents\\github\\wbudowane-systemy-ai\\.venv\\lib\\site-packages (from tensorflow-metadata->tensorflow-datasets) (1.61.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install --upgrade tensorrt\n",
        "!pip install tensorflow-datasets\n",
        "import tensorflow as tf\n",
        "import tensorrt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ErsrtA8dfLU"
      },
      "source": [
        "Before we start, chech if the GPU is available (we going to need it!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "D7lAmE8MdbFa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "print(tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZuhOF30TX-9"
      },
      "source": [
        "Today, we are going to try a bit bigger model and harder dataset. We'll try running Imagenet classification with MobileNet network.\n",
        "\n",
        "More about the dataset: https://www.image-net.org/\n",
        "\n",
        "More about the network: https://keras.io/api/applications/mobilenet/\n",
        "\n",
        "It would take some time to train this model, so we'll just use ready, pretrained neural network. Use `tf.keras.applications.MobileNet()` function to create `CNN` instance (just study the link above). Use `include_top=True`, `weights=\"imagenet\"` and `classes=1000`. Based on documentation answer the question - what is model's input size and what is it's output size?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_VYYPAsaTp0V"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf.h5\n",
            "17225924/17225924 [==============================] - 3s 0us/step\n"
          ]
        }
      ],
      "source": [
        "CNN = tf.keras.applications.MobileNet(include_top=True, weights='imagenet', classes=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqckVEc6Xkog"
      },
      "source": [
        "Now, let's download the dataset with `tfds` module. Use `tfds.load()` function with `imagenet_v2` dataset name (this is the dataset used for MobileNet training), `split='test[70%:]` (we need just 3000 samples), and `shuffle_files=True` and `as_supervised=True` parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cnaAnDMsovx"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "# Now, let's download the dataset with `tfds` module.\n",
        "ds = tfds.load('imagenet_v2', split='test[70%:]', shuffle_files=True, as_supervised=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO7dEYvVYNfl"
      },
      "source": [
        "Perfect. Now, we have both pretrained model and test dataset ready. We can benchmark the model. Implement benchmarking similarly as in previous lab, but:\n",
        "- calulate not only throughput, and TOP1 accuracy, but also TOP5 accuracy (is correct label found in 5 classes with highest prediction probability?). The `((-preds[0]).argsort()[:5])` function may prove useful here.\n",
        "- you can loop through dataset with `for image, label in tfds.as_numpy(ds):`\n",
        "- each image should be resized to model input size and then reshaped to `(1,input_size, input_size, nr_of_channels)` before `predict` function. Just before model's input use `preprocess_input()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP-tGiaTsVD3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.mobilenet import preprocess_input, decode_predictions\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "N_warmup_run = 50\n",
        "N_run = 500\n",
        "total_time = 0\n",
        "correct_top1_predictions = 0\n",
        "correct_top5_predictions = 0\n",
        "\n",
        "for i, (image, label) in enumerate(tfds.as_numpy(ds)):\n",
        "    # Warm up runs\n",
        "    if i < N_warmup_run:\n",
        "        continue\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Preprocess the image and resize it to the correct input size for the model\n",
        "    image = cv2.resize(image, (224, 224))\n",
        "    image = preprocess_input(image)\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "\n",
        "    # Make predictions\n",
        "    preds = CNN.predict(image)\n",
        "    \n",
        "    # Calculate TOP1 accuracy\n",
        "    top1_pred_label = np.argmax(preds[0])\n",
        "    correct_top1_predictions += (top1_pred_label == label)\n",
        "\n",
        "    # Calculate TOP5 accuracy\n",
        "    top5_pred_labels = ((-preds[0]).argsort()[:5])\n",
        "    correct_top5_predictions += (label in top5_pred_labels)\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_time += (end_time - start_time)\n",
        "\n",
        "    # Break the loop after N_run\n",
        "    if i >= (N_run + N_warmup_run):\n",
        "        break\n",
        "\n",
        "# Calculate throughput, TOP1 accuracy, and TOP5 accuracy\n",
        "throughput = N_run / total_time\n",
        "top1_accuracy = correct_top1_predictions / N_run\n",
        "top5_accuracy = correct_top5_predictions / N_run\n",
        "\n",
        "print('Throughput: {:.2f} images/sec'.format(throughput))\n",
        "print('TOP1 accuracy: {:.2f}%'.format(top1_accuracy * 100))\n",
        "print('TOP5 accuracy: {:.2f}%'.format(top5_accuracy * 100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocBxRmZ3X2bX"
      },
      "source": [
        "After benchmarking - save model with `CNN.save()`. We got familiar with MobileNet and ImageNet. Now we can carry on with Jetson Nano. Show this part of exercise to the teacher and ask for Jeston Nano board."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zm-88TKxXGay"
      },
      "outputs": [],
      "source": [
        "SAVED_MODEL_DIR=\"saved_model\"\n",
        "CNN.save(SAVED_MODEL_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rskpRVbTapDb"
      },
      "source": [
        "Our task is to run inference on Jetson Nano with TensorRT model. You are going to:\n",
        "1. Prepare the Jetson Nano board.\n",
        "2. Convert Mobilenet with TensorRT.\n",
        "3. Run inference on example image.\n",
        "4. **Extended** Benchmark Jetson Nano inference.\n",
        "5. **Extension exercise** Connect camera and run live classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8opMWVOcReD"
      },
      "source": [
        "1. Prepare the Jetson Nano board\n",
        "- First, take Jetson Nano board, connect it to power source, internet, monitor, mouse and keyboard.\n",
        "- Log in to Jetson and finish OS instalation (the boards were not used yet).\n",
        "- Open terminal and add cuda to PATH `export PATH=$PATH:/usr/local/cuda-10/bin`. Verify CUDA with `nvcc --version`.\n",
        "- Download NVIDIA Tensorflow docker `sudo docker pull nvcr.io/nvidia/l4t-tensorflow:r32.7.1-tf2.7-py3`\n",
        "- Run NVIDIA Tensorflow docker with `sudo docker run -it --rm --runtime nvidia --network host -v /home/nano/Documents:/home/ nvcr.io/nvidia/l4t-tensorflow:r32.7.1-tf2.7-py3`. Now, you can use TensorRT, TensorFlow and CUDA in this terminal window. Moreover, the path `/home/nano/Documents/` outside of docker is linked to `/home/` inside of docker.\n",
        "- Copy Notebook from this lab to `/home/nano/Documents/` \n",
        "- Being in docker terminal run the Notebook\n",
        "\n",
        "1. Convert MobileNet with TensorRT\n",
        "\n",
        "Use the code from Lab6 to convert `MobileNet` model to TensorRT. Start with FP32 model (to make it simple). You can move `save_model` created here to Jetson or create CNN on Jetson (`tf.keras.applications.MobileNet()`)\n",
        "\n",
        "3. Run inference on example image.\n",
        "- Warmup model with dummy imput (`np.random`)\n",
        "- Pass image to `infer` and analize network output.\n",
        "- You can get example images from imagenet from `https://github.com/EliSchwartz/imagenet-sample-images.git` repository. Use PIL to read and resize images.\n",
        "\n",
        "4. **Extension exercise** - Loop thorugh all images, benchmark TensorRT model.\n",
        "\n",
        "5. **Extension exercise** - Implement simple application that uses USB camera and TensorRT for live classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
